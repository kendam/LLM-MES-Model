{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.0\n",
      "numpy version: 1.25.2\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.1\n",
      "tensorflow version: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "643a7c49-ce71-4df9-a0de-2473b69d7181",
   "metadata": {},
   "source": [
    "-Here, we implement the training loop and code for basic model evaluation to pretrain an LLM\n",
    "-Later, we also load openly available pretrained weights from OpenAI into our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd27fcc-2886-47cb-b544-046c2c31f02a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67711d4-8391-4fee-aeef-07ea53dd5841",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d824183-145c-4865-89e1-1f0d0a338f19",
   "metadata": {
    "id": "0d824183-145c-4865-89e1-1f0d0a338f19"
   },
   "source": [
    "Evaluating generative text models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350f8c-5181-4f9b-a789-4523105e98f2",
   "metadata": {},
   "source": [
    "- We start this section with a brief recap of initializing a GPT model using the code from the utils\n",
    "- Then, we discuss basic evaluation metrics for LLMs\n",
    "- Lastly, in this section, we apply these evaluation metrics to a training and validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd",
   "metadata": {
    "id": "bdc1cf3f-82d8-46c7-9ecc-58979ce87cdd"
   },
   "source": [
    "Using GPT to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3415fd-9f4a-4548-908e-9dfa56edc9bc",
   "metadata": {},
   "source": [
    "- We initialize a GPT model using the code from the util\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6cf0f-7458-48a2-97fd-aa5068d65e8c",
   "metadata": {},
   "source": [
    "- We use dropout of 0.1 above, but it's relatively common to train LLMs without dropout nowadays\n",
    "- Modern LLMs also don't use bias vectors in the `nn.Linear` layers for the query, key, and value matrices (unlike earlier GPT models), which is achieved by setting `\"qkv_bias\": False`\n",
    "- We reduce the context length (`context_length`) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "  - This is so that more readers will be able to follow and execute the code examples on their laptop computer\n",
    "  - However, please feel free to increase the `context_length` to 1024 tokens (this would not require any code changes)\n",
    "  - We will also load a model with a 1024 `context_length` later from pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80895-be35-4bb5-81cb-f357ef7367fe",
   "metadata": {},
   "source": [
    "- Next, we use the `generate_text_simple` function from the previous utils to generate text\n",
    "- In addition, we define two convenience functions, `text_to_token_ids` and `token_ids_to_text`, for converting between token and text representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741881f3-cee0-49ad-b11d-b9df3b3ac234",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-process.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from utils import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3249b-b2a0-44c4-b589-ae4b403b8305",
   "metadata": {},
   "source": [
    "- As we can see above, the model does not produce good text because it has not been trained yet\n",
    "- How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "- The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "- on finetuning LLMs will also introduce additional ways to measure model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9e1a-7bf7-40d8-b1fa-eacabdee8d8e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
   "metadata": {
    "id": "0f3d7ea2-637f-4490-bc76-e361fc81ae98",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Calculating the text generation loss: cross entropy, and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ba8aa-fb03-4d25-957f-fe8778762440",
   "metadata": {},
   "source": [
    "- Suppose we have an `inputs` tensor containing the token IDs for 2 training examples (rows)\n",
    "- Corresponding to the `inputs`, the `targets` contain the desired token IDs that we want the model to generate\n",
    "- Notice that the `targets` are the `inputs` shifted by 1 position, as explained in chapter 2 when we implemented the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5402f8-ec0c-4a44-9892-18a97779ee4f",
    "outputId": "8d6fa0ff-7b37-4634-c3f0-2c050cbe81f0"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc0645-ac2c-4973-9b40-6da40515bede",
   "metadata": {},
   "source": [
    "- Feeding the `inputs` to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each\n",
    "- Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n",
    "- Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b6ec51-6f8c-49bd-a349-95ba38b46fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36a382-b5e2-4de6-9e65-0b69b685013b",
   "metadata": {},
   "source": [
    "- The figure below, using a very small vocabulary for illustration purposes, outlines how we convert the probability scores back into text, which we discussed at the end of the previous chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d86a9-0013-476c-bb6b-274fd5f20b29",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-to-text.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480efd-d419-4954-9ecc-2876055334bd",
   "metadata": {},
   "source": [
    "- As discussed in the previous chapter, we can apply the `argmax` function to convert the probability scores into predicted token IDs\n",
    "- The softmax function above produced a 50,257-dimensional vector for each token; the `argmax` function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b84c9f-dd08-482e-b903-a86fe44e1144",
   "metadata": {},
   "source": [
    "- Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34ebd76a-16ec-4c17-8958-8a135735cc1c",
    "outputId": "ed17da47-c3e7-4775-fd00-4ec5bcda3db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4072c-21ed-4df7-8721-dd2535362573",
   "metadata": {},
   "source": [
    "- If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c990ead6-53cd-49a7-a6d1-14d8c1518249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eb8a7-070e-46d6-930c-314ba55a6ff2",
   "metadata": {},
   "source": [
    "- That's because the model wasn't trained yet\n",
    "- To train the model, we need to know how far it is away from the correct predictions (targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90592f-0d5d-4ec8-9ff5-e7675beab10e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251bf5-a079-4782-901d-68c9225d3157",
   "metadata": {},
   "source": [
    "- The token probabilities corresponding to the target indices are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54aef09c-d6e3-4238-8653-b3a1b0a1077a",
    "outputId": "41c946a2-c458-433e-a53d-5e7e89d9dddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89a19-73c2-4e49-93b4-861f699f1cbf",
   "metadata": {},
   "source": [
    "- We want to maximize all these values, bringing them close to a probability of 1\n",
    "- In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself; this is out of the scope of this book, but I have recorded a lecture with more details here: [L8.2 Logistic Regression Loss Function](https://www.youtube.com/watch?v=GxJe0DZvydM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31402a67-a16e-4aeb-977e-70abb9c9949b",
    "outputId": "1bf18e79-1246-4eab-efd8-12b328c78678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4261441-a511-4633-9c4c-67998af31b84",
   "metadata": {},
   "source": [
    "- Next, we compute the average log probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b003797-161b-4d98-81dc-e68320e09fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b003797-161b-4d98-81dc-e68320e09fec",
    "outputId": "a447fe9c-7e27-40ed-f1fb-51210e3f7cc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d51994-ad17-4ba3-a6ec-f588b4b13585",
   "metadata": {},
   "source": [
    "- The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    "- Due to the log, the largest possible value is 0, and we are currently far away from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de388a1-8a0a-4c94-8894-9041dc6ad514",
   "metadata": {},
   "source": [
    "- In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the *negative* average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0\n",
    "- The value negative of -10.7722, i.e., 10.7722, is also called cross entropy loss in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "176ddf35-1c5f-4d7c-bf17-70f3e7069bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eeb868-abd8-4028-82db-107546bf7c2c",
   "metadata": {},
   "source": [
    "- PyTorch already implements a `cross_entropy` function that carries out the previous steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24b7f-b760-47ad-bc84-86d13794aa54",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/cross-entropy.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf9dd-3ee6-42bf-a63f-6e93dbfb989d",
   "metadata": {},
   "source": [
    "- Before we apply the cross entropy function, let's check the shape of the logits and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695d6f64-5084-4c23-aea4-105c9e38cfe4",
    "outputId": "43fd802a-8136-4b35-df0d-f61a5d4cb561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d65f0-6566-4865-93e4-0c0bcb10cd06",
   "metadata": {},
   "source": [
    "- For the cross `entropy_loss` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e17e027-ab9f-4fb5-ac9b-a009b831c122",
    "outputId": "0b2b778b-02fb-43b2-c879-adc59055a7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921a57f-3a79-473e-a863-6d63b495010f",
   "metadata": {},
   "source": [
    "- Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize\n",
    "- The `cross_entropy` function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62d0816e-b29a-4c8f-a9a5-a167562de978",
    "outputId": "c0be634a-2c65-4ff7-a73f-1bfc2e406ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15ce17-fd7b-4d8e-99da-b237523a7a80",
   "metadata": {},
   "source": [
    "- A concept related to the cross entropy loss is the perplexity of an LLM\n",
    "- The perplexity is simply the exponential of the cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "168952a1-b964-4aa7-8e49-966fa26add54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "168952a1-b964-4aa7-8e49-966fa26add54",
    "outputId": "a0a692c1-6412-4068-8aa5-8858548141eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae26dd-d77e-41fd-b924-6bd103dd4ee7",
   "metadata": {},
   "source": [
    "- The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 47,678 words or tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530da89e-2448-436c-8f1b-28e8a31ef85c",
   "metadata": {},
   "source": [
    "- We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "- The reasons are:\n",
    "  - You can run the code examples in a few minutes on a laptop computer without a suitable GPU\n",
    "  - The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes\n",
    "  - We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size\n",
    "\n",
    "\n",
    "- For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n",
    "  - At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately \\\\$30\n",
    "  - So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * \\\\$30 =  \\\\$690,000\n",
    " \n",
    "- Below, we use the same dataset we used in chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"mobley-second-edit.txt\"\n",
    "url = \"https://raw.githubusercontent.com/kendam/LLM-MES/refs/heads/main/data/Training/pred-maintenance.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- A quick check that the text loaded ok by printing the first and last 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPACT OF MAINTENANCE\n",
      "Maintenance costs are a major part of the total operating costs of all man\n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " employment and training information to sponsors and the local employment and training community.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 901412\n",
      "Tokens: 186724\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8830cb9-90f6-4e7c-8620-beeabc2d39f7",
   "metadata": {},
   "source": [
    "- With 186,724 tokens, the text short enough for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcad87-a0e8-4b9d-ac43-4e927ccbb50f",
   "metadata": {},
   "source": [
    "- Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training\n",
    "- For visualization purposes, the figure below assumes a `max_length=6`, but for the training loader, we set the `max_length` equal to the context length that the LLM supports\n",
    "- The figure below only shows the input tokens for simplicity\n",
    "    - Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdaa07-ba96-4ac1-9d71-b3cc153910d9",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/batching.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f37b3eb0-854e-4895-9898-fa7d1e67566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac3296-a4d1-4303-9ac5-376518960c33",
   "metadata": {},
   "source": [
    "- We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with\n",
    "- Llama 2 7B was trained with a batch size of 1024, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0514d-b990-4dc0-9afb-7721993284a0",
   "metadata": {},
   "source": [
    "- An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9b1a4-863d-456f-a8dd-c07fb5c024ed",
   "metadata": {},
   "source": [
    "- Another optional check that the token sizes are in the expected ballpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 168960\n",
      "Validation tokens: 17408\n",
      "All tokens: 186368\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3085e8-665e-48eb-bb41-cdde61537e06",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross entropy loss of a given batch\n",
    "- In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc",
   "metadata": {
    "id": "7b9de31e-4096-47b3-976d-b6d2fdce04bc"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0691332-84d0-48b3-b462-a885ddeb4fca",
   "metadata": {},
   "source": [
    "- If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code\n",
    "- Via the `device` setting, we ensure that the data is loaded onto the same device as the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.981392227519642\n",
      "Validation loss: 10.958945218254538\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875e95-190f-4b17-8f9a-35034ba649ec",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-1.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a4cf4-e98f-46d9-bdec-60e7ccb8d6bd",
   "metadata": {},
   "source": [
    "- In this section, we finally implement the code for training the LLM\n",
    "- We focus on a simple training function (if you are interested in augmenting this training function with more advanced techniques, such as learning rate warmup, cosine annealing, and gradient clipping, please refer to [Appendix D](../../appendix-D/01_main-chapter-code))\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "- Now, let's train the LLM using the training function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.986, Val loss 9.926\n",
      "Ep 1 (Step 000005): Train loss 8.509, Val loss 8.458\n",
      "Ep 1 (Step 000010): Train loss 7.975, Val loss 7.537\n",
      "Ep 1 (Step 000015): Train loss 7.336, Val loss 7.257\n",
      "Ep 1 (Step 000020): Train loss 7.612, Val loss 7.393\n",
      "Ep 1 (Step 000025): Train loss 7.323, Val loss 7.413\n",
      "Ep 1 (Step 000030): Train loss 7.094, Val loss 7.321\n",
      "Ep 1 (Step 000035): Train loss 7.217, Val loss 7.180\n",
      "Ep 1 (Step 000040): Train loss 6.849, Val loss 7.095\n",
      "Ep 1 (Step 000045): Train loss 7.290, Val loss 7.034\n",
      "Ep 1 (Step 000050): Train loss 6.856, Val loss 7.040\n",
      "Ep 1 (Step 000055): Train loss 6.788, Val loss 7.050\n",
      "Ep 1 (Step 000060): Train loss 6.435, Val loss 6.965\n",
      "Ep 1 (Step 000065): Train loss 6.557, Val loss 6.956\n",
      "Ep 1 (Step 000070): Train loss 6.583, Val loss 6.907\n",
      "Ep 1 (Step 000075): Train loss 6.496, Val loss 6.828\n",
      "Ep 1 (Step 000080): Train loss 6.605, Val loss 6.791\n",
      "Ep 1 (Step 000085): Train loss 6.378, Val loss 6.766\n",
      "Ep 1 (Step 000090): Train loss 6.332, Val loss 6.758\n",
      "Ep 1 (Step 000095): Train loss 6.273, Val loss 6.791\n",
      "Ep 1 (Step 000100): Train loss 6.315, Val loss 6.784\n",
      "Ep 1 (Step 000105): Train loss 6.731, Val loss 6.744\n",
      "Ep 1 (Step 000110): Train loss 6.175, Val loss 6.725\n",
      "Ep 1 (Step 000115): Train loss 6.293, Val loss 6.675\n",
      "Ep 1 (Step 000120): Train loss 6.317, Val loss 6.668\n",
      "Ep 1 (Step 000125): Train loss 6.317, Val loss 6.583\n",
      "Ep 1 (Step 000130): Train loss 6.299, Val loss 6.597\n",
      "Ep 1 (Step 000135): Train loss 6.245, Val loss 6.565\n",
      "Ep 1 (Step 000140): Train loss 6.108, Val loss 6.562\n",
      "Ep 1 (Step 000145): Train loss 6.065, Val loss 6.524\n",
      "Ep 1 (Step 000150): Train loss 6.080, Val loss 6.530\n",
      "Ep 1 (Step 000155): Train loss 6.324, Val loss 6.551\n",
      "Ep 1 (Step 000160): Train loss 5.955, Val loss 6.534\n",
      "Ep 1 (Step 000165): Train loss 5.836, Val loss 6.489\n",
      "Ep 1 (Step 000170): Train loss 6.139, Val loss 6.502\n",
      "Ep 1 (Step 000175): Train loss 6.308, Val loss 6.454\n",
      "Ep 1 (Step 000180): Train loss 5.966, Val loss 6.443\n",
      "Ep 1 (Step 000185): Train loss 5.892, Val loss 6.440\n",
      "Ep 1 (Step 000190): Train loss 6.139, Val loss 6.414\n",
      "Ep 1 (Step 000195): Train loss 5.792, Val loss 6.396\n",
      "Ep 1 (Step 000200): Train loss 5.819, Val loss 6.329\n",
      "Ep 1 (Step 000205): Train loss 5.747, Val loss 6.329\n",
      "Ep 1 (Step 000210): Train loss 5.925, Val loss 6.330\n",
      "Ep 1 (Step 000215): Train loss 5.874, Val loss 6.350\n",
      "Ep 1 (Step 000220): Train loss 5.510, Val loss 6.405\n",
      "Ep 1 (Step 000225): Train loss 5.854, Val loss 6.417\n",
      "Ep 1 (Step 000230): Train loss 5.971, Val loss 6.408\n",
      "Ep 1 (Step 000235): Train loss 6.059, Val loss 6.383\n",
      "Ep 1 (Step 000240): Train loss 5.715, Val loss 6.351\n",
      "Ep 1 (Step 000245): Train loss 5.851, Val loss 6.419\n",
      "Ep 1 (Step 000250): Train loss 5.931, Val loss 6.414\n",
      "Ep 1 (Step 000255): Train loss 5.771, Val loss 6.412\n",
      "Ep 1 (Step 000260): Train loss 5.864, Val loss 6.409\n",
      "Ep 1 (Step 000265): Train loss 5.880, Val loss 6.346\n",
      "Ep 1 (Step 000270): Train loss 6.007, Val loss 6.306\n",
      "Ep 1 (Step 000275): Train loss 5.654, Val loss 6.374\n",
      "Ep 1 (Step 000280): Train loss 5.615, Val loss 6.308\n",
      "Ep 1 (Step 000285): Train loss 5.861, Val loss 6.345\n",
      "Ep 1 (Step 000290): Train loss 5.967, Val loss 6.339\n",
      "Ep 1 (Step 000295): Train loss 5.533, Val loss 6.314\n",
      "Ep 1 (Step 000300): Train loss 5.481, Val loss 6.323\n",
      "Ep 1 (Step 000305): Train loss 5.305, Val loss 6.307\n",
      "Ep 1 (Step 000310): Train loss 5.865, Val loss 6.308\n",
      "Ep 1 (Step 000315): Train loss 5.633, Val loss 6.275\n",
      "Ep 1 (Step 000320): Train loss 5.734, Val loss 6.375\n",
      "Ep 1 (Step 000325): Train loss 5.459, Val loss 6.285\n",
      " Ã¯ maintenance, and the pump,ce, and to the pump.\n",
      "Ep 2 (Step 000330): Train loss 5.218, Val loss 6.232\n",
      "Ep 2 (Step 000335): Train loss 5.584, Val loss 6.269\n",
      "Ep 2 (Step 000340): Train loss 5.346, Val loss 6.291\n",
      "Ep 2 (Step 000345): Train loss 5.648, Val loss 6.347\n",
      "Ep 2 (Step 000350): Train loss 5.684, Val loss 6.292\n",
      "Ep 2 (Step 000355): Train loss 5.488, Val loss 6.232\n",
      "Ep 2 (Step 000360): Train loss 5.521, Val loss 6.270\n",
      "Ep 2 (Step 000365): Train loss 5.608, Val loss 6.239\n",
      "Ep 2 (Step 000370): Train loss 5.476, Val loss 6.242\n",
      "Ep 2 (Step 000375): Train loss 5.955, Val loss 6.268\n",
      "Ep 2 (Step 000380): Train loss 5.731, Val loss 6.233\n",
      "Ep 2 (Step 000385): Train loss 5.428, Val loss 6.252\n",
      "Ep 2 (Step 000390): Train loss 5.213, Val loss 6.219\n",
      "Ep 2 (Step 000395): Train loss 5.596, Val loss 6.185\n",
      "Ep 2 (Step 000400): Train loss 5.723, Val loss 6.177\n",
      "Ep 2 (Step 000405): Train loss 5.428, Val loss 6.175\n",
      "Ep 2 (Step 000410): Train loss 5.495, Val loss 6.197\n",
      "Ep 2 (Step 000415): Train loss 5.522, Val loss 6.216\n",
      "Ep 2 (Step 000420): Train loss 5.344, Val loss 6.204\n",
      "Ep 2 (Step 000425): Train loss 5.502, Val loss 6.209\n",
      "Ep 2 (Step 000430): Train loss 5.345, Val loss 6.174\n",
      "Ep 2 (Step 000435): Train loss 5.369, Val loss 6.167\n",
      "Ep 2 (Step 000440): Train loss 5.519, Val loss 6.193\n",
      "Ep 2 (Step 000445): Train loss 5.487, Val loss 6.162\n",
      "Ep 2 (Step 000450): Train loss 5.343, Val loss 6.134\n",
      "Ep 2 (Step 000455): Train loss 5.315, Val loss 6.196\n",
      "Ep 2 (Step 000460): Train loss 5.331, Val loss 6.130\n",
      "Ep 2 (Step 000465): Train loss 5.386, Val loss 6.130\n",
      "Ep 2 (Step 000470): Train loss 5.664, Val loss 6.205\n",
      "Ep 2 (Step 000475): Train loss 5.015, Val loss 6.244\n",
      "Ep 2 (Step 000480): Train loss 5.443, Val loss 6.231\n",
      "Ep 2 (Step 000485): Train loss 5.167, Val loss 6.184\n",
      "Ep 2 (Step 000490): Train loss 5.381, Val loss 6.165\n",
      "Ep 2 (Step 000495): Train loss 5.051, Val loss 6.146\n",
      "Ep 2 (Step 000500): Train loss 5.506, Val loss 6.164\n",
      "Ep 2 (Step 000505): Train loss 5.176, Val loss 6.179\n",
      "Ep 2 (Step 000510): Train loss 4.906, Val loss 6.165\n",
      "Ep 2 (Step 000515): Train loss 5.208, Val loss 6.116\n",
      "Ep 2 (Step 000520): Train loss 5.256, Val loss 6.100\n",
      "Ep 2 (Step 000525): Train loss 4.957, Val loss 6.125\n",
      "Ep 2 (Step 000530): Train loss 4.947, Val loss 6.113\n",
      "Ep 2 (Step 000535): Train loss 5.191, Val loss 6.031\n",
      "Ep 2 (Step 000540): Train loss 4.945, Val loss 6.045\n",
      "Ep 2 (Step 000545): Train loss 5.259, Val loss 6.050\n",
      "Ep 2 (Step 000550): Train loss 5.081, Val loss 6.031\n",
      "Ep 2 (Step 000555): Train loss 5.114, Val loss 6.059\n",
      "Ep 2 (Step 000560): Train loss 5.014, Val loss 6.084\n",
      "Ep 2 (Step 000565): Train loss 5.283, Val loss 6.054\n",
      "Ep 2 (Step 000570): Train loss 5.440, Val loss 6.049\n",
      "Ep 2 (Step 000575): Train loss 5.339, Val loss 5.992\n",
      "Ep 2 (Step 000580): Train loss 5.142, Val loss 5.966\n",
      "Ep 2 (Step 000585): Train loss 4.832, Val loss 6.017\n",
      "Ep 2 (Step 000590): Train loss 5.065, Val loss 6.009\n",
      "Ep 2 (Step 000595): Train loss 5.016, Val loss 6.025\n",
      "Ep 2 (Step 000600): Train loss 5.126, Val loss 6.037\n",
      "Ep 2 (Step 000605): Train loss 4.997, Val loss 6.062\n",
      "Ep 2 (Step 000610): Train loss 5.134, Val loss 6.011\n",
      "Ep 2 (Step 000615): Train loss 5.002, Val loss 6.033\n",
      "Ep 2 (Step 000620): Train loss 5.274, Val loss 6.033\n",
      "Ep 2 (Step 000625): Train loss 4.991, Val loss 5.997\n",
      "Ep 2 (Step 000630): Train loss 5.101, Val loss 5.977\n",
      "Ep 2 (Step 000635): Train loss 5.003, Val loss 6.050\n",
      "Ep 2 (Step 000640): Train loss 4.988, Val loss 6.058\n",
      "Ep 2 (Step 000645): Train loss 5.189, Val loss 5.979\n",
      "Ep 2 (Step 000650): Train loss 5.240, Val loss 5.946\n",
      "Ep 2 (Step 000655): Train loss 4.895, Val loss 5.970\n",
      " Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³Ã³\n",
      "Ep 3 (Step 000660): Train loss 4.861, Val loss 5.970\n",
      "Ep 3 (Step 000665): Train loss 5.269, Val loss 5.982\n",
      "Ep 3 (Step 000670): Train loss 4.585, Val loss 6.017\n",
      "Ep 3 (Step 000675): Train loss 4.828, Val loss 6.014\n",
      "Ep 3 (Step 000680): Train loss 4.855, Val loss 5.949\n",
      "Ep 3 (Step 000685): Train loss 4.834, Val loss 5.974\n",
      "Ep 3 (Step 000690): Train loss 5.206, Val loss 5.948\n",
      "Ep 3 (Step 000695): Train loss 4.886, Val loss 5.952\n",
      "Ep 3 (Step 000700): Train loss 4.937, Val loss 5.996\n",
      "Ep 3 (Step 000705): Train loss 4.806, Val loss 5.941\n",
      "Ep 3 (Step 000710): Train loss 4.906, Val loss 5.941\n",
      "Ep 3 (Step 000715): Train loss 4.975, Val loss 5.942\n",
      "Ep 3 (Step 000720): Train loss 5.107, Val loss 5.989\n",
      "Ep 3 (Step 000725): Train loss 4.930, Val loss 5.952\n",
      "Ep 3 (Step 000730): Train loss 4.958, Val loss 5.960\n",
      "Ep 3 (Step 000735): Train loss 4.859, Val loss 5.944\n",
      "Ep 3 (Step 000740): Train loss 4.740, Val loss 5.970\n",
      "Ep 3 (Step 000745): Train loss 4.785, Val loss 5.971\n",
      "Ep 3 (Step 000750): Train loss 4.710, Val loss 5.956\n",
      "Ep 3 (Step 000755): Train loss 4.646, Val loss 5.877\n",
      "Ep 3 (Step 000760): Train loss 4.940, Val loss 5.974\n",
      "Ep 3 (Step 000765): Train loss 4.887, Val loss 5.889\n",
      "Ep 3 (Step 000770): Train loss 4.741, Val loss 5.820\n",
      "Ep 3 (Step 000775): Train loss 4.993, Val loss 5.782\n",
      "Ep 3 (Step 000780): Train loss 5.022, Val loss 5.856\n",
      "Ep 3 (Step 000785): Train loss 4.943, Val loss 5.871\n",
      "Ep 3 (Step 000790): Train loss 4.533, Val loss 5.842\n",
      "Ep 3 (Step 000795): Train loss 4.615, Val loss 5.819\n",
      "Ep 3 (Step 000800): Train loss 4.996, Val loss 5.814\n",
      "Ep 3 (Step 000805): Train loss 5.022, Val loss 5.808\n",
      "Ep 3 (Step 000810): Train loss 4.937, Val loss 5.814\n",
      "Ep 3 (Step 000815): Train loss 4.711, Val loss 5.809\n",
      "Ep 3 (Step 000820): Train loss 5.030, Val loss 5.760\n",
      "Ep 3 (Step 000825): Train loss 4.488, Val loss 5.824\n",
      "Ep 3 (Step 000830): Train loss 4.622, Val loss 5.807\n",
      "Ep 3 (Step 000835): Train loss 4.538, Val loss 5.799\n",
      "Ep 3 (Step 000840): Train loss 4.842, Val loss 5.756\n",
      "Ep 3 (Step 000845): Train loss 4.979, Val loss 5.783\n",
      "Ep 3 (Step 000850): Train loss 5.147, Val loss 5.874\n",
      "Ep 3 (Step 000855): Train loss 4.979, Val loss 5.795\n",
      "Ep 3 (Step 000860): Train loss 4.444, Val loss 5.780\n",
      "Ep 3 (Step 000865): Train loss 4.652, Val loss 5.804\n",
      "Ep 3 (Step 000870): Train loss 4.639, Val loss 5.827\n",
      "Ep 3 (Step 000875): Train loss 4.739, Val loss 5.853\n",
      "Ep 3 (Step 000880): Train loss 4.844, Val loss 5.852\n",
      "Ep 3 (Step 000885): Train loss 4.561, Val loss 5.803\n",
      "Ep 3 (Step 000890): Train loss 4.470, Val loss 5.812\n",
      "Ep 3 (Step 000895): Train loss 4.940, Val loss 5.810\n",
      "Ep 3 (Step 000900): Train loss 4.775, Val loss 5.752\n",
      "Ep 3 (Step 000905): Train loss 4.930, Val loss 5.758\n",
      "Ep 3 (Step 000910): Train loss 4.606, Val loss 5.809\n",
      "Ep 3 (Step 000915): Train loss 4.183, Val loss 5.848\n",
      "Ep 3 (Step 000920): Train loss 4.669, Val loss 5.783\n",
      "Ep 3 (Step 000925): Train loss 4.747, Val loss 5.765\n",
      "Ep 3 (Step 000930): Train loss 4.507, Val loss 5.734\n",
      "Ep 3 (Step 000935): Train loss 4.497, Val loss 5.753\n",
      "Ep 3 (Step 000940): Train loss 4.432, Val loss 5.779\n",
      "Ep 3 (Step 000945): Train loss 4.728, Val loss 5.750\n",
      "Ep 3 (Step 000950): Train loss 4.473, Val loss 5.717\n",
      "Ep 3 (Step 000955): Train loss 4.751, Val loss 5.744\n",
      "Ep 3 (Step 000960): Train loss 4.601, Val loss 5.766\n",
      "Ep 3 (Step 000965): Train loss 4.372, Val loss 5.745\n",
      "Ep 3 (Step 000970): Train loss 4.704, Val loss 5.735\n",
      "Ep 3 (Step 000975): Train loss 4.604, Val loss 5.814\n",
      "Ep 3 (Step 000980): Train loss 4.655, Val loss 5.775\n",
      "Ep 3 (Step 000985): Train loss 4.348, Val loss 5.742\n",
      " Ã¯ Estive maintenance program.based data.\n",
      "Ep 4 (Step 000990): Train loss 4.451, Val loss 5.846\n",
      "Ep 4 (Step 000995): Train loss 4.577, Val loss 5.818\n",
      "Ep 4 (Step 001000): Train loss 4.711, Val loss 5.777\n",
      "Ep 4 (Step 001005): Train loss 4.334, Val loss 5.782\n",
      "Ep 4 (Step 001010): Train loss 4.572, Val loss 5.763\n",
      "Ep 4 (Step 001015): Train loss 4.426, Val loss 5.777\n",
      "Ep 4 (Step 001020): Train loss 4.487, Val loss 5.766\n",
      "Ep 4 (Step 001025): Train loss 4.512, Val loss 5.780\n",
      "Ep 4 (Step 001030): Train loss 4.152, Val loss 5.764\n",
      "Ep 4 (Step 001035): Train loss 4.671, Val loss 5.783\n",
      "Ep 4 (Step 001040): Train loss 4.245, Val loss 5.790\n",
      "Ep 4 (Step 001045): Train loss 4.734, Val loss 5.731\n",
      "Ep 4 (Step 001050): Train loss 4.357, Val loss 5.774\n",
      "Ep 4 (Step 001055): Train loss 4.550, Val loss 5.779\n",
      "Ep 4 (Step 001060): Train loss 4.530, Val loss 5.784\n",
      "Ep 4 (Step 001065): Train loss 4.415, Val loss 5.762\n",
      "Ep 4 (Step 001070): Train loss 4.670, Val loss 5.802\n",
      "Ep 4 (Step 001075): Train loss 4.312, Val loss 5.794\n",
      "Ep 4 (Step 001080): Train loss 4.671, Val loss 5.760\n",
      "Ep 4 (Step 001085): Train loss 4.785, Val loss 5.760\n",
      "Ep 4 (Step 001090): Train loss 4.549, Val loss 5.774\n",
      "Ep 4 (Step 001095): Train loss 4.055, Val loss 5.739\n",
      "Ep 4 (Step 001100): Train loss 4.287, Val loss 5.745\n",
      "Ep 4 (Step 001105): Train loss 4.493, Val loss 5.751\n",
      "Ep 4 (Step 001110): Train loss 3.896, Val loss 5.760\n",
      "Ep 4 (Step 001115): Train loss 4.669, Val loss 5.746\n",
      "Ep 4 (Step 001120): Train loss 4.312, Val loss 5.745\n",
      "Ep 4 (Step 001125): Train loss 4.463, Val loss 5.728\n",
      "Ep 4 (Step 001130): Train loss 4.510, Val loss 5.748\n",
      "Ep 4 (Step 001135): Train loss 4.405, Val loss 5.720\n",
      "Ep 4 (Step 001140): Train loss 4.412, Val loss 5.731\n",
      "Ep 4 (Step 001145): Train loss 4.247, Val loss 5.786\n",
      "Ep 4 (Step 001150): Train loss 4.100, Val loss 5.769\n",
      "Ep 4 (Step 001155): Train loss 4.167, Val loss 5.741\n",
      "Ep 4 (Step 001160): Train loss 4.187, Val loss 5.720\n",
      "Ep 4 (Step 001165): Train loss 4.683, Val loss 5.764\n",
      "Ep 4 (Step 001170): Train loss 4.292, Val loss 5.759\n",
      "Ep 4 (Step 001175): Train loss 4.583, Val loss 5.738\n",
      "Ep 4 (Step 001180): Train loss 4.140, Val loss 5.706\n",
      "Ep 4 (Step 001185): Train loss 4.139, Val loss 5.784\n",
      "Ep 4 (Step 001190): Train loss 4.221, Val loss 5.740\n",
      "Ep 4 (Step 001195): Train loss 4.171, Val loss 5.728\n",
      "Ep 4 (Step 001200): Train loss 4.235, Val loss 5.777\n",
      "Ep 4 (Step 001205): Train loss 4.269, Val loss 5.786\n",
      "Ep 4 (Step 001210): Train loss 4.338, Val loss 5.730\n",
      "Ep 4 (Step 001215): Train loss 4.394, Val loss 5.777\n",
      "Ep 4 (Step 001220): Train loss 4.193, Val loss 5.795\n",
      "Ep 4 (Step 001225): Train loss 4.382, Val loss 5.823\n",
      "Ep 4 (Step 001230): Train loss 4.371, Val loss 5.755\n",
      "Ep 4 (Step 001235): Train loss 4.310, Val loss 5.698\n",
      "Ep 4 (Step 001240): Train loss 4.262, Val loss 5.702\n",
      "Ep 4 (Step 001245): Train loss 4.454, Val loss 5.817\n",
      "Ep 4 (Step 001250): Train loss 4.340, Val loss 5.711\n",
      "Ep 4 (Step 001255): Train loss 4.082, Val loss 5.696\n",
      "Ep 4 (Step 001260): Train loss 4.370, Val loss 5.722\n",
      "Ep 4 (Step 001265): Train loss 4.071, Val loss 5.711\n",
      "Ep 4 (Step 001270): Train loss 3.956, Val loss 5.702\n",
      "Ep 4 (Step 001275): Train loss 4.135, Val loss 5.627\n",
      "Ep 4 (Step 001280): Train loss 4.068, Val loss 5.632\n",
      "Ep 4 (Step 001285): Train loss 4.192, Val loss 5.640\n",
      "Ep 4 (Step 001290): Train loss 4.156, Val loss 5.627\n",
      "Ep 4 (Step 001295): Train loss 4.209, Val loss 5.614\n",
      "Ep 4 (Step 001300): Train loss 3.990, Val loss 5.624\n",
      "Ep 4 (Step 001305): Train loss 4.395, Val loss 5.613\n",
      "Ep 4 (Step 001310): Train loss 4.061, Val loss 5.642\n",
      "Ep 4 (Step 001315): Train loss 4.131, Val loss 5.687\n",
      " The actual operating condition of the plant.\n",
      "Ep 5 (Step 001320): Train loss 4.203, Val loss 5.665\n",
      "Ep 5 (Step 001325): Train loss 3.878, Val loss 5.654\n",
      "Ep 5 (Step 001330): Train loss 4.077, Val loss 5.708\n",
      "Ep 5 (Step 001335): Train loss 4.225, Val loss 5.516\n",
      "Ep 5 (Step 001340): Train loss 4.201, Val loss 5.512\n",
      "Ep 5 (Step 001345): Train loss 3.927, Val loss 5.590\n",
      "Ep 5 (Step 001350): Train loss 4.076, Val loss 5.581\n",
      "Ep 5 (Step 001355): Train loss 4.221, Val loss 5.568\n",
      "Ep 5 (Step 001360): Train loss 3.981, Val loss 5.650\n",
      "Ep 5 (Step 001365): Train loss 4.163, Val loss 5.656\n",
      "Ep 5 (Step 001370): Train loss 4.005, Val loss 5.653\n",
      "Ep 5 (Step 001375): Train loss 3.753, Val loss 5.674\n",
      "Ep 5 (Step 001380): Train loss 3.812, Val loss 5.604\n",
      "Ep 5 (Step 001385): Train loss 3.930, Val loss 5.599\n",
      "Ep 5 (Step 001390): Train loss 3.783, Val loss 5.707\n",
      "Ep 5 (Step 001395): Train loss 3.704, Val loss 5.700\n",
      "Ep 5 (Step 001400): Train loss 4.081, Val loss 5.724\n",
      "Ep 5 (Step 001405): Train loss 4.114, Val loss 5.664\n",
      "Ep 5 (Step 001410): Train loss 4.290, Val loss 5.693\n",
      "Ep 5 (Step 001415): Train loss 3.539, Val loss 5.718\n",
      "Ep 5 (Step 001420): Train loss 4.097, Val loss 5.702\n",
      "Ep 5 (Step 001425): Train loss 4.169, Val loss 5.719\n",
      "Ep 5 (Step 001430): Train loss 4.125, Val loss 5.667\n",
      "Ep 5 (Step 001435): Train loss 3.880, Val loss 5.636\n",
      "Ep 5 (Step 001440): Train loss 3.686, Val loss 5.649\n",
      "Ep 5 (Step 001445): Train loss 3.991, Val loss 5.683\n",
      "Ep 5 (Step 001450): Train loss 4.088, Val loss 5.665\n",
      "Ep 5 (Step 001455): Train loss 4.142, Val loss 5.668\n",
      "Ep 5 (Step 001460): Train loss 3.901, Val loss 5.688\n",
      "Ep 5 (Step 001465): Train loss 3.870, Val loss 5.680\n",
      "Ep 5 (Step 001470): Train loss 3.927, Val loss 5.652\n",
      "Ep 5 (Step 001475): Train loss 4.076, Val loss 5.695\n",
      "Ep 5 (Step 001480): Train loss 3.895, Val loss 5.677\n",
      "Ep 5 (Step 001485): Train loss 4.326, Val loss 5.703\n",
      "Ep 5 (Step 001490): Train loss 3.853, Val loss 5.689\n",
      "Ep 5 (Step 001495): Train loss 3.986, Val loss 5.674\n",
      "Ep 5 (Step 001500): Train loss 3.812, Val loss 5.665\n",
      "Ep 5 (Step 001505): Train loss 3.825, Val loss 5.668\n",
      "Ep 5 (Step 001510): Train loss 3.822, Val loss 5.691\n",
      "Ep 5 (Step 001515): Train loss 4.187, Val loss 5.679\n",
      "Ep 5 (Step 001520): Train loss 4.100, Val loss 5.669\n",
      "Ep 5 (Step 001525): Train loss 3.804, Val loss 5.662\n",
      "Ep 5 (Step 001530): Train loss 4.005, Val loss 5.720\n",
      "Ep 5 (Step 001535): Train loss 3.969, Val loss 5.668\n",
      "Ep 5 (Step 001540): Train loss 3.917, Val loss 5.673\n",
      "Ep 5 (Step 001545): Train loss 3.753, Val loss 5.756\n",
      "Ep 5 (Step 001550): Train loss 3.801, Val loss 5.740\n",
      "Ep 5 (Step 001555): Train loss 3.624, Val loss 5.707\n",
      "Ep 5 (Step 001560): Train loss 3.895, Val loss 5.715\n",
      "Ep 5 (Step 001565): Train loss 4.185, Val loss 5.711\n",
      "Ep 5 (Step 001570): Train loss 3.967, Val loss 5.693\n",
      "Ep 5 (Step 001575): Train loss 3.604, Val loss 5.710\n",
      "Ep 5 (Step 001580): Train loss 3.727, Val loss 5.693\n",
      "Ep 5 (Step 001585): Train loss 3.694, Val loss 5.701\n",
      "Ep 5 (Step 001590): Train loss 3.880, Val loss 5.687\n",
      "Ep 5 (Step 001595): Train loss 3.684, Val loss 5.670\n",
      "Ep 5 (Step 001600): Train loss 3.865, Val loss 5.677\n",
      "Ep 5 (Step 001605): Train loss 3.768, Val loss 5.738\n",
      "Ep 5 (Step 001610): Train loss 3.763, Val loss 5.778\n",
      "Ep 5 (Step 001615): Train loss 3.929, Val loss 5.723\n",
      "Ep 5 (Step 001620): Train loss 3.714, Val loss 5.729\n",
      "Ep 5 (Step 001625): Train loss 3.707, Val loss 5.749\n",
      "Ep 5 (Step 001630): Train loss 3.511, Val loss 5.725\n",
      "Ep 5 (Step 001635): Train loss 3.816, Val loss 5.714\n",
      "Ep 5 (Step 001640): Train loss 3.790, Val loss 5.731\n",
      "Ep 5 (Step 001645): Train loss 3.924, Val loss 5.697\n",
      " The microprocessor-based analyzers that a single-based vibration dataingle-term vibration data acquisition data acquisition data acquisition data acquisition.\n",
      "Ep 6 (Step 001650): Train loss 3.758, Val loss 5.688\n",
      "Ep 6 (Step 001655): Train loss 3.603, Val loss 5.731\n",
      "Ep 6 (Step 001660): Train loss 3.865, Val loss 5.699\n",
      "Ep 6 (Step 001665): Train loss 3.782, Val loss 5.719\n",
      "Ep 6 (Step 001670): Train loss 3.609, Val loss 5.767\n",
      "Ep 6 (Step 001675): Train loss 3.662, Val loss 5.693\n",
      "Ep 6 (Step 001680): Train loss 3.608, Val loss 5.711\n",
      "Ep 6 (Step 001685): Train loss 3.737, Val loss 5.695\n",
      "Ep 6 (Step 001690): Train loss 3.547, Val loss 5.751\n",
      "Ep 6 (Step 001695): Train loss 3.615, Val loss 5.734\n",
      "Ep 6 (Step 001700): Train loss 3.866, Val loss 5.727\n",
      "Ep 6 (Step 001705): Train loss 4.009, Val loss 5.764\n",
      "Ep 6 (Step 001710): Train loss 3.657, Val loss 5.757\n",
      "Ep 6 (Step 001715): Train loss 3.819, Val loss 5.798\n",
      "Ep 6 (Step 001720): Train loss 3.571, Val loss 5.818\n",
      "Ep 6 (Step 001725): Train loss 3.708, Val loss 5.778\n",
      "Ep 6 (Step 001730): Train loss 3.726, Val loss 5.764\n",
      "Ep 6 (Step 001735): Train loss 3.387, Val loss 5.764\n",
      "Ep 6 (Step 001740): Train loss 3.301, Val loss 5.812\n",
      "Ep 6 (Step 001745): Train loss 3.729, Val loss 5.855\n",
      "Ep 6 (Step 001750): Train loss 3.535, Val loss 5.820\n",
      "Ep 6 (Step 001755): Train loss 3.442, Val loss 5.825\n",
      "Ep 6 (Step 001760): Train loss 3.564, Val loss 5.805\n",
      "Ep 6 (Step 001765): Train loss 3.781, Val loss 5.823\n",
      "Ep 6 (Step 001770): Train loss 3.622, Val loss 5.778\n",
      "Ep 6 (Step 001775): Train loss 3.805, Val loss 5.759\n",
      "Ep 6 (Step 001780): Train loss 3.789, Val loss 5.815\n",
      "Ep 6 (Step 001785): Train loss 3.309, Val loss 5.759\n",
      "Ep 6 (Step 001790): Train loss 3.632, Val loss 5.703\n",
      "Ep 6 (Step 001795): Train loss 3.506, Val loss 5.764\n",
      "Ep 6 (Step 001800): Train loss 3.674, Val loss 5.803\n",
      "Ep 6 (Step 001805): Train loss 3.609, Val loss 5.760\n",
      "Ep 6 (Step 001810): Train loss 3.445, Val loss 5.799\n",
      "Ep 6 (Step 001815): Train loss 3.666, Val loss 5.688\n",
      "Ep 6 (Step 001820): Train loss 3.435, Val loss 5.667\n",
      "Ep 6 (Step 001825): Train loss 3.345, Val loss 5.734\n",
      "Ep 6 (Step 001830): Train loss 3.422, Val loss 5.696\n",
      "Ep 6 (Step 001835): Train loss 3.368, Val loss 5.731\n",
      "Ep 6 (Step 001840): Train loss 3.658, Val loss 5.739\n",
      "Ep 6 (Step 001845): Train loss 3.486, Val loss 5.736\n",
      "Ep 6 (Step 001850): Train loss 3.583, Val loss 5.693\n",
      "Ep 6 (Step 001855): Train loss 3.658, Val loss 5.644\n",
      "Ep 6 (Step 001860): Train loss 3.605, Val loss 5.625\n",
      "Ep 6 (Step 001865): Train loss 3.596, Val loss 5.678\n",
      "Ep 6 (Step 001870): Train loss 3.453, Val loss 5.699\n",
      "Ep 6 (Step 001875): Train loss 3.570, Val loss 5.685\n",
      "Ep 6 (Step 001880): Train loss 3.381, Val loss 5.657\n",
      "Ep 6 (Step 001885): Train loss 3.522, Val loss 5.711\n",
      "Ep 6 (Step 001890): Train loss 3.715, Val loss 5.686\n",
      "Ep 6 (Step 001895): Train loss 3.338, Val loss 5.693\n",
      "Ep 6 (Step 001900): Train loss 3.529, Val loss 5.698\n",
      "Ep 6 (Step 001905): Train loss 3.319, Val loss 5.733\n",
      "Ep 6 (Step 001910): Train loss 3.383, Val loss 5.786\n",
      "Ep 6 (Step 001915): Train loss 3.319, Val loss 5.783\n",
      "Ep 6 (Step 001920): Train loss 3.344, Val loss 5.771\n",
      "Ep 6 (Step 001925): Train loss 3.519, Val loss 5.773\n",
      "Ep 6 (Step 001930): Train loss 3.640, Val loss 5.720\n",
      "Ep 6 (Step 001935): Train loss 3.441, Val loss 5.736\n",
      "Ep 6 (Step 001940): Train loss 3.418, Val loss 5.737\n",
      "Ep 6 (Step 001945): Train loss 3.430, Val loss 5.728\n",
      "Ep 6 (Step 001950): Train loss 3.507, Val loss 5.715\n",
      "Ep 6 (Step 001955): Train loss 3.556, Val loss 5.723\n",
      "Ep 6 (Step 001960): Train loss 3.293, Val loss 5.723\n",
      "Ep 6 (Step 001965): Train loss 3.320, Val loss 5.729\n",
      "Ep 6 (Step 001970): Train loss 3.334, Val loss 5.739\n",
      "Ep 6 (Step 001975): Train loss 3.387, Val loss 5.717\n",
      " Ã¯ Calibrationdts are a major interval.\n",
      "Ep 7 (Step 001980): Train loss 3.236, Val loss 5.722\n",
      "Ep 7 (Step 001985): Train loss 3.292, Val loss 5.683\n",
      "Ep 7 (Step 001990): Train loss 3.239, Val loss 5.658\n",
      "Ep 7 (Step 001995): Train loss 3.469, Val loss 5.657\n",
      "Ep 7 (Step 002000): Train loss 3.432, Val loss 5.686\n",
      "Ep 7 (Step 002005): Train loss 3.371, Val loss 5.695\n",
      "Ep 7 (Step 002010): Train loss 3.174, Val loss 5.698\n",
      "Ep 7 (Step 002015): Train loss 3.255, Val loss 5.660\n",
      "Ep 7 (Step 002020): Train loss 3.455, Val loss 5.715\n",
      "Ep 7 (Step 002025): Train loss 3.059, Val loss 5.779\n",
      "Ep 7 (Step 002030): Train loss 3.162, Val loss 5.786\n",
      "Ep 7 (Step 002035): Train loss 3.262, Val loss 5.778\n",
      "Ep 7 (Step 002040): Train loss 3.207, Val loss 5.776\n",
      "Ep 7 (Step 002045): Train loss 3.342, Val loss 5.819\n",
      "Ep 7 (Step 002050): Train loss 2.975, Val loss 5.817\n",
      "Ep 7 (Step 002055): Train loss 3.397, Val loss 5.746\n",
      "Ep 7 (Step 002060): Train loss 3.401, Val loss 5.739\n",
      "Ep 7 (Step 002065): Train loss 3.246, Val loss 5.764\n",
      "Ep 7 (Step 002070): Train loss 3.319, Val loss 5.794\n",
      "Ep 7 (Step 002075): Train loss 3.179, Val loss 5.807\n",
      "Ep 7 (Step 002080): Train loss 3.477, Val loss 5.797\n",
      "Ep 7 (Step 002085): Train loss 3.207, Val loss 5.769\n",
      "Ep 7 (Step 002090): Train loss 3.069, Val loss 5.814\n",
      "Ep 7 (Step 002095): Train loss 3.154, Val loss 5.849\n",
      "Ep 7 (Step 002100): Train loss 3.068, Val loss 5.819\n",
      "Ep 7 (Step 002105): Train loss 3.360, Val loss 5.831\n",
      "Ep 7 (Step 002110): Train loss 3.283, Val loss 5.818\n",
      "Ep 7 (Step 002115): Train loss 2.978, Val loss 5.834\n",
      "Ep 7 (Step 002120): Train loss 3.492, Val loss 5.837\n",
      "Ep 7 (Step 002125): Train loss 3.420, Val loss 5.866\n",
      "Ep 7 (Step 002130): Train loss 3.335, Val loss 5.886\n",
      "Ep 7 (Step 002135): Train loss 3.392, Val loss 5.877\n",
      "Ep 7 (Step 002140): Train loss 3.215, Val loss 5.840\n",
      "Ep 7 (Step 002145): Train loss 3.281, Val loss 5.817\n",
      "Ep 7 (Step 002150): Train loss 3.348, Val loss 5.835\n",
      "Ep 7 (Step 002155): Train loss 3.473, Val loss 5.834\n",
      "Ep 7 (Step 002160): Train loss 3.141, Val loss 5.847\n",
      "Ep 7 (Step 002165): Train loss 2.965, Val loss 5.845\n",
      "Ep 7 (Step 002170): Train loss 2.997, Val loss 5.913\n",
      "Ep 7 (Step 002175): Train loss 3.055, Val loss 5.899\n",
      "Ep 7 (Step 002180): Train loss 3.273, Val loss 5.877\n",
      "Ep 7 (Step 002185): Train loss 3.213, Val loss 5.836\n",
      "Ep 7 (Step 002190): Train loss 3.062, Val loss 5.833\n",
      "Ep 7 (Step 002195): Train loss 3.146, Val loss 5.854\n",
      "Ep 7 (Step 002200): Train loss 3.332, Val loss 5.866\n",
      "Ep 7 (Step 002205): Train loss 3.016, Val loss 5.869\n",
      "Ep 7 (Step 002210): Train loss 2.984, Val loss 5.851\n",
      "Ep 7 (Step 002215): Train loss 3.204, Val loss 5.853\n",
      "Ep 7 (Step 002220): Train loss 3.270, Val loss 5.876\n",
      "Ep 7 (Step 002225): Train loss 2.882, Val loss 5.824\n",
      "Ep 7 (Step 002230): Train loss 2.847, Val loss 5.760\n",
      "Ep 7 (Step 002235): Train loss 3.153, Val loss 5.766\n",
      "Ep 7 (Step 002240): Train loss 3.206, Val loss 5.764\n",
      "Ep 7 (Step 002245): Train loss 3.002, Val loss 5.746\n",
      "Ep 7 (Step 002250): Train loss 2.843, Val loss 5.793\n",
      "Ep 7 (Step 002255): Train loss 2.816, Val loss 5.808\n",
      "Ep 7 (Step 002260): Train loss 3.067, Val loss 5.784\n",
      "Ep 7 (Step 002265): Train loss 3.188, Val loss 5.812\n",
      "Ep 7 (Step 002270): Train loss 2.808, Val loss 5.829\n",
      "Ep 7 (Step 002275): Train loss 3.058, Val loss 5.791\n",
      "Ep 7 (Step 002280): Train loss 2.766, Val loss 5.809\n",
      "Ep 7 (Step 002285): Train loss 3.102, Val loss 5.801\n",
      "Ep 7 (Step 002290): Train loss 2.964, Val loss 5.753\n",
      "Ep 7 (Step 002295): Train loss 3.165, Val loss 5.777\n",
      "Ep 7 (Step 002300): Train loss 3.187, Val loss 5.859\n",
      "Ep 7 (Step 002305): Train loss 2.957, Val loss 5.844\n",
      " The survey, and other program.thod of the first step is the first step is not a result of a predictive maintenance program. The software program. The software program. The software program.\n",
      "Ep 8 (Step 002310): Train loss 3.174, Val loss 5.807\n",
      "Ep 8 (Step 002315): Train loss 2.860, Val loss 5.809\n",
      "Ep 8 (Step 002320): Train loss 2.966, Val loss 5.833\n",
      "Ep 8 (Step 002325): Train loss 2.884, Val loss 5.864\n",
      "Ep 8 (Step 002330): Train loss 3.027, Val loss 5.912\n",
      "Ep 8 (Step 002335): Train loss 3.094, Val loss 5.883\n",
      "Ep 8 (Step 002340): Train loss 2.852, Val loss 5.860\n",
      "Ep 8 (Step 002345): Train loss 3.095, Val loss 5.916\n",
      "Ep 8 (Step 002350): Train loss 3.059, Val loss 5.905\n",
      "Ep 8 (Step 002355): Train loss 2.770, Val loss 5.885\n",
      "Ep 8 (Step 002360): Train loss 3.073, Val loss 5.942\n",
      "Ep 8 (Step 002365): Train loss 2.746, Val loss 5.896\n",
      "Ep 8 (Step 002370): Train loss 2.770, Val loss 5.830\n",
      "Ep 8 (Step 002375): Train loss 2.943, Val loss 5.888\n",
      "Ep 8 (Step 002380): Train loss 2.710, Val loss 5.908\n",
      "Ep 8 (Step 002385): Train loss 2.828, Val loss 5.861\n",
      "Ep 8 (Step 002390): Train loss 2.947, Val loss 5.905\n",
      "Ep 8 (Step 002395): Train loss 2.816, Val loss 5.939\n",
      "Ep 8 (Step 002400): Train loss 2.750, Val loss 5.903\n",
      "Ep 8 (Step 002405): Train loss 2.453, Val loss 5.877\n",
      "Ep 8 (Step 002410): Train loss 3.038, Val loss 5.902\n",
      "Ep 8 (Step 002415): Train loss 2.819, Val loss 5.936\n",
      "Ep 8 (Step 002420): Train loss 2.529, Val loss 5.954\n",
      "Ep 8 (Step 002425): Train loss 2.826, Val loss 5.948\n",
      "Ep 8 (Step 002430): Train loss 2.765, Val loss 5.917\n",
      "Ep 8 (Step 002435): Train loss 3.263, Val loss 5.957\n",
      "Ep 8 (Step 002440): Train loss 2.800, Val loss 5.953\n",
      "Ep 8 (Step 002445): Train loss 2.617, Val loss 5.921\n",
      "Ep 8 (Step 002450): Train loss 2.871, Val loss 5.930\n",
      "Ep 8 (Step 002455): Train loss 2.810, Val loss 5.942\n",
      "Ep 8 (Step 002460): Train loss 3.014, Val loss 5.952\n",
      "Ep 8 (Step 002465): Train loss 2.764, Val loss 5.889\n",
      "Ep 8 (Step 002470): Train loss 2.754, Val loss 5.917\n",
      "Ep 8 (Step 002475): Train loss 2.781, Val loss 5.917\n",
      "Ep 8 (Step 002480): Train loss 2.806, Val loss 5.938\n",
      "Ep 8 (Step 002485): Train loss 2.738, Val loss 5.937\n",
      "Ep 8 (Step 002490): Train loss 3.085, Val loss 5.891\n",
      "Ep 8 (Step 002495): Train loss 2.641, Val loss 5.918\n",
      "Ep 8 (Step 002500): Train loss 3.032, Val loss 5.903\n",
      "Ep 8 (Step 002505): Train loss 2.874, Val loss 5.952\n",
      "Ep 8 (Step 002510): Train loss 2.591, Val loss 5.965\n",
      "Ep 8 (Step 002515): Train loss 2.942, Val loss 5.966\n",
      "Ep 8 (Step 002520): Train loss 2.661, Val loss 5.990\n",
      "Ep 8 (Step 002525): Train loss 2.648, Val loss 6.000\n",
      "Ep 8 (Step 002530): Train loss 2.905, Val loss 5.967\n",
      "Ep 8 (Step 002535): Train loss 2.596, Val loss 5.980\n",
      "Ep 8 (Step 002540): Train loss 2.657, Val loss 5.943\n",
      "Ep 8 (Step 002545): Train loss 2.712, Val loss 5.967\n",
      "Ep 8 (Step 002550): Train loss 2.497, Val loss 5.954\n",
      "Ep 8 (Step 002555): Train loss 2.498, Val loss 5.893\n",
      "Ep 8 (Step 002560): Train loss 2.885, Val loss 5.901\n",
      "Ep 8 (Step 002565): Train loss 2.473, Val loss 5.901\n",
      "Ep 8 (Step 002570): Train loss 2.760, Val loss 5.897\n",
      "Ep 8 (Step 002575): Train loss 2.740, Val loss 5.929\n",
      "Ep 8 (Step 002580): Train loss 2.886, Val loss 5.923\n",
      "Ep 8 (Step 002585): Train loss 2.677, Val loss 5.853\n",
      "Ep 8 (Step 002590): Train loss 2.888, Val loss 5.887\n",
      "Ep 8 (Step 002595): Train loss 2.508, Val loss 5.886\n",
      "Ep 8 (Step 002600): Train loss 2.610, Val loss 5.876\n",
      "Ep 8 (Step 002605): Train loss 2.536, Val loss 5.897\n",
      "Ep 8 (Step 002610): Train loss 2.607, Val loss 5.907\n",
      "Ep 8 (Step 002615): Train loss 2.698, Val loss 5.900\n",
      "Ep 8 (Step 002620): Train loss 2.700, Val loss 5.900\n",
      "Ep 8 (Step 002625): Train loss 2.597, Val loss 5.910\n",
      "Ep 8 (Step 002630): Train loss 2.723, Val loss 5.944\n",
      "Ep 8 (Step 002635): Train loss 2.794, Val loss 5.992\n",
      " The cost of the cost of the cost of the cost of the cost of the cost of the cost of the previousple, or more than 1,000Hz, and imbalance.\n",
      "Ep 9 (Step 002640): Train loss 2.609, Val loss 5.924\n",
      "Ep 9 (Step 002645): Train loss 2.586, Val loss 5.927\n",
      "Ep 9 (Step 002650): Train loss 2.434, Val loss 6.019\n",
      "Ep 9 (Step 002655): Train loss 2.524, Val loss 5.945\n",
      "Ep 9 (Step 002660): Train loss 2.591, Val loss 5.948\n",
      "Ep 9 (Step 002665): Train loss 2.604, Val loss 5.930\n",
      "Ep 9 (Step 002670): Train loss 2.484, Val loss 5.939\n",
      "Ep 9 (Step 002675): Train loss 2.645, Val loss 5.973\n",
      "Ep 9 (Step 002680): Train loss 2.572, Val loss 5.985\n",
      "Ep 9 (Step 002685): Train loss 2.291, Val loss 6.014\n",
      "Ep 9 (Step 002690): Train loss 2.821, Val loss 6.004\n",
      "Ep 9 (Step 002695): Train loss 2.488, Val loss 5.999\n",
      "Ep 9 (Step 002700): Train loss 2.542, Val loss 6.058\n",
      "Ep 9 (Step 002705): Train loss 2.583, Val loss 6.074\n",
      "Ep 9 (Step 002710): Train loss 2.568, Val loss 6.035\n",
      "Ep 9 (Step 002715): Train loss 2.641, Val loss 6.041\n",
      "Ep 9 (Step 002720): Train loss 2.542, Val loss 6.076\n",
      "Ep 9 (Step 002725): Train loss 2.631, Val loss 6.076\n",
      "Ep 9 (Step 002730): Train loss 2.637, Val loss 6.053\n",
      "Ep 9 (Step 002735): Train loss 2.442, Val loss 6.063\n",
      "Ep 9 (Step 002740): Train loss 2.567, Val loss 6.098\n",
      "Ep 9 (Step 002745): Train loss 2.674, Val loss 6.024\n",
      "Ep 9 (Step 002750): Train loss 2.512, Val loss 6.018\n",
      "Ep 9 (Step 002755): Train loss 2.228, Val loss 6.033\n",
      "Ep 9 (Step 002760): Train loss 2.503, Val loss 6.115\n",
      "Ep 9 (Step 002765): Train loss 2.521, Val loss 6.114\n",
      "Ep 9 (Step 002770): Train loss 2.606, Val loss 6.091\n",
      "Ep 9 (Step 002775): Train loss 2.595, Val loss 6.112\n",
      "Ep 9 (Step 002780): Train loss 2.432, Val loss 6.135\n",
      "Ep 9 (Step 002785): Train loss 2.427, Val loss 6.083\n",
      "Ep 9 (Step 002790): Train loss 2.474, Val loss 6.103\n",
      "Ep 9 (Step 002795): Train loss 2.743, Val loss 6.106\n",
      "Ep 9 (Step 002800): Train loss 2.462, Val loss 6.138\n",
      "Ep 9 (Step 002805): Train loss 2.338, Val loss 6.117\n",
      "Ep 9 (Step 002810): Train loss 2.148, Val loss 6.139\n",
      "Ep 9 (Step 002815): Train loss 2.446, Val loss 6.180\n",
      "Ep 9 (Step 002820): Train loss 2.449, Val loss 6.160\n",
      "Ep 9 (Step 002825): Train loss 2.564, Val loss 6.120\n",
      "Ep 9 (Step 002830): Train loss 2.203, Val loss 6.133\n",
      "Ep 9 (Step 002835): Train loss 2.484, Val loss 6.136\n",
      "Ep 9 (Step 002840): Train loss 2.441, Val loss 6.127\n",
      "Ep 9 (Step 002845): Train loss 2.418, Val loss 6.143\n",
      "Ep 9 (Step 002850): Train loss 2.323, Val loss 6.116\n",
      "Ep 9 (Step 002855): Train loss 2.323, Val loss 6.117\n",
      "Ep 9 (Step 002860): Train loss 2.050, Val loss 6.141\n",
      "Ep 9 (Step 002865): Train loss 2.577, Val loss 6.144\n",
      "Ep 9 (Step 002870): Train loss 2.639, Val loss 6.116\n",
      "Ep 9 (Step 002875): Train loss 2.641, Val loss 6.168\n",
      "Ep 9 (Step 002880): Train loss 2.541, Val loss 6.126\n",
      "Ep 9 (Step 002885): Train loss 2.392, Val loss 6.139\n",
      "Ep 9 (Step 002890): Train loss 2.421, Val loss 6.139\n",
      "Ep 9 (Step 002895): Train loss 2.197, Val loss 6.160\n",
      "Ep 9 (Step 002900): Train loss 2.644, Val loss 6.187\n",
      "Ep 9 (Step 002905): Train loss 2.204, Val loss 6.147\n",
      "Ep 9 (Step 002910): Train loss 2.261, Val loss 6.162\n",
      "Ep 9 (Step 002915): Train loss 2.476, Val loss 6.159\n",
      "Ep 9 (Step 002920): Train loss 2.418, Val loss 6.153\n",
      "Ep 9 (Step 002925): Train loss 2.389, Val loss 6.110\n",
      "Ep 9 (Step 002930): Train loss 2.335, Val loss 6.052\n",
      "Ep 9 (Step 002935): Train loss 2.466, Val loss 6.067\n",
      "Ep 9 (Step 002940): Train loss 2.353, Val loss 6.048\n",
      "Ep 9 (Step 002945): Train loss 2.497, Val loss 6.035\n",
      "Ep 9 (Step 002950): Train loss 2.562, Val loss 6.056\n",
      "Ep 9 (Step 002955): Train loss 2.116, Val loss 6.033\n",
      "Ep 9 (Step 002960): Train loss 2.142, Val loss 6.035\n",
      "Ep 9 (Step 002965): Train loss 2.393, Val loss 6.016\n",
      "Maintenance costs are a major classifications. The amount of the amount of the amount of the amount of the pumpÃ­s hydraulic curve, the pumpÃ­s hydraulic, the pumpÃ­s operating condition of pumpÃ­s hydraulic curve. The pumpÃ­s hydraulic curve. The amount to the pumpÃ­s hydraulic\n",
      "Ep 10 (Step 002970): Train loss 2.354, Val loss 6.016\n",
      "Ep 10 (Step 002975): Train loss 2.307, Val loss 6.025\n",
      "Ep 10 (Step 002980): Train loss 2.191, Val loss 6.053\n",
      "Ep 10 (Step 002985): Train loss 2.075, Val loss 6.071\n",
      "Ep 10 (Step 002990): Train loss 2.338, Val loss 6.136\n",
      "Ep 10 (Step 002995): Train loss 2.141, Val loss 6.191\n",
      "Ep 10 (Step 003000): Train loss 2.328, Val loss 6.186\n",
      "Ep 10 (Step 003005): Train loss 2.463, Val loss 6.191\n",
      "Ep 10 (Step 003010): Train loss 2.323, Val loss 6.229\n",
      "Ep 10 (Step 003015): Train loss 2.239, Val loss 6.205\n",
      "Ep 10 (Step 003020): Train loss 2.214, Val loss 6.153\n",
      "Ep 10 (Step 003025): Train loss 2.066, Val loss 6.174\n",
      "Ep 10 (Step 003030): Train loss 2.041, Val loss 6.237\n",
      "Ep 10 (Step 003035): Train loss 2.035, Val loss 6.268\n",
      "Ep 10 (Step 003040): Train loss 2.321, Val loss 6.271\n",
      "Ep 10 (Step 003045): Train loss 2.077, Val loss 6.266\n",
      "Ep 10 (Step 003050): Train loss 2.048, Val loss 6.270\n",
      "Ep 10 (Step 003055): Train loss 2.367, Val loss 6.284\n",
      "Ep 10 (Step 003060): Train loss 1.986, Val loss 6.269\n",
      "Ep 10 (Step 003065): Train loss 2.391, Val loss 6.289\n",
      "Ep 10 (Step 003070): Train loss 1.848, Val loss 6.283\n",
      "Ep 10 (Step 003075): Train loss 2.172, Val loss 6.318\n",
      "Ep 10 (Step 003080): Train loss 2.321, Val loss 6.256\n",
      "Ep 10 (Step 003085): Train loss 2.216, Val loss 6.264\n",
      "Ep 10 (Step 003090): Train loss 2.190, Val loss 6.245\n",
      "Ep 10 (Step 003095): Train loss 1.998, Val loss 6.247\n",
      "Ep 10 (Step 003100): Train loss 2.335, Val loss 6.251\n",
      "Ep 10 (Step 003105): Train loss 2.201, Val loss 6.256\n",
      "Ep 10 (Step 003110): Train loss 2.208, Val loss 6.235\n",
      "Ep 10 (Step 003115): Train loss 2.083, Val loss 6.245\n",
      "Ep 10 (Step 003120): Train loss 2.326, Val loss 6.307\n",
      "Ep 10 (Step 003125): Train loss 2.257, Val loss 6.294\n",
      "Ep 10 (Step 003130): Train loss 2.168, Val loss 6.211\n",
      "Ep 10 (Step 003135): Train loss 2.084, Val loss 6.212\n",
      "Ep 10 (Step 003140): Train loss 2.153, Val loss 6.179\n",
      "Ep 10 (Step 003145): Train loss 1.873, Val loss 6.208\n",
      "Ep 10 (Step 003150): Train loss 1.973, Val loss 6.270\n",
      "Ep 10 (Step 003155): Train loss 2.134, Val loss 6.266\n",
      "Ep 10 (Step 003160): Train loss 1.996, Val loss 6.293\n",
      "Ep 10 (Step 003165): Train loss 2.242, Val loss 6.316\n",
      "Ep 10 (Step 003170): Train loss 2.076, Val loss 6.271\n",
      "Ep 10 (Step 003175): Train loss 2.101, Val loss 6.246\n",
      "Ep 10 (Step 003180): Train loss 2.085, Val loss 6.230\n",
      "Ep 10 (Step 003185): Train loss 1.916, Val loss 6.220\n",
      "Ep 10 (Step 003190): Train loss 1.968, Val loss 6.227\n",
      "Ep 10 (Step 003195): Train loss 2.105, Val loss 6.263\n",
      "Ep 10 (Step 003200): Train loss 2.104, Val loss 6.262\n",
      "Ep 10 (Step 003205): Train loss 1.821, Val loss 6.229\n",
      "Ep 10 (Step 003210): Train loss 1.918, Val loss 6.185\n",
      "Ep 10 (Step 003215): Train loss 1.921, Val loss 6.207\n",
      "Ep 10 (Step 003220): Train loss 1.963, Val loss 6.209\n",
      "Ep 10 (Step 003225): Train loss 2.058, Val loss 6.208\n",
      "Ep 10 (Step 003230): Train loss 2.223, Val loss 6.206\n",
      "Ep 10 (Step 003235): Train loss 1.910, Val loss 6.218\n",
      "Ep 10 (Step 003240): Train loss 1.821, Val loss 6.190\n",
      "Ep 10 (Step 003245): Train loss 1.928, Val loss 6.218\n",
      "Ep 10 (Step 003250): Train loss 2.098, Val loss 6.253\n",
      "Ep 10 (Step 003255): Train loss 2.129, Val loss 6.254\n",
      "Ep 10 (Step 003260): Train loss 1.925, Val loss 6.284\n",
      "Ep 10 (Step 003265): Train loss 1.984, Val loss 6.224\n",
      "Ep 10 (Step 003270): Train loss 1.765, Val loss 6.205\n",
      "Ep 10 (Step 003275): Train loss 2.009, Val loss 6.226\n",
      "Ep 10 (Step 003280): Train loss 1.953, Val loss 6.209\n",
      "Ep 10 (Step 003285): Train loss 1.826, Val loss 6.195\n",
      "Ep 10 (Step 003290): Train loss 1.892, Val loss 6.236\n",
      "Ep 10 (Step 003295): Train loss 1.981, Val loss 6.266\n",
      " Single-plane imbalance is designed by the rotor with the rotor assembly. The other insol%) of the rotor to the rotor, the rotor speed (i.\n",
      "Training completed in 754.71 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Maintenance costs are a major\", tokenizer=tokenizer\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAEiCAYAAADd4SrgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABvGUlEQVR4nO3dd1xV5R/A8c9l7ylTBEVRFBG3Kc4kR2qOyiwzVzbU1GyYP3M21DIz0zQbWrmynFnu3Dv3niguxIUsWfee3x9HLlw2CHLB7/v14iX3nOec85x75X7PszWKoigIIYQQwuiYlHQGhBBCCJE9CdJCCCGEkZIgLYQQQhgpCdJCCCGEkZIgLYQQQhgpCdJCCCGEkZIgLYQQQhgpCdJCCCGEkZIgLYQQQhgpCdJCCCGEkZIgLYQQ4omxbds2OnXqhLe3NxqNhhUrVhT4HIqiMGXKFKpWrYqlpSXly5fns88+K/rMIkFaiFLl0qVLaDQaDh8+XNJZEaJUio+PJyQkhJkzZxb6HEOHDuXHH39kypQpnD59mlWrVtGwYcMizGU6s2I5qxAiRxqNJtf9Y8eOZdy4cY8nM0I8Ydq3b0/79u1z3J+UlMSoUaNYtGgR0dHR1KxZk8mTJ9OyZUsATp06xaxZszh+/DjVqlUDoFKlSsWWXwnSQjxmN27c0P/++++/M2bMGM6cOaPfZmdnVxLZEkIAgwcP5uTJkyxevBhvb2+WL19Ou3btOHbsGAEBAfz111/4+/uzevVq2rVrh6IohIWF8cUXX+Di4lLk+ZHqbiEeM09PT/2Po6MjGo1G/9rd3Z2pU6fi4+ODpaUltWvXZu3atTmeS6vV0q9fPwIDA4mIiABg5cqV1K1bFysrK/z9/Rk/fjypqan6YzQaDT/++CNdu3bFxsaGgIAAVq1apd9/7949evbsiZubG9bW1gQEBDB37twc8/Dnn38SHByMtbU1rq6uhIWFER8fr9//448/Ur16daysrAgMDOS7774zOP7KlSt0794dJycnXFxc6Ny5M5cuXdLv79OnD126dGHKlCl4eXnh6urKoEGDSElJyfd7LkR+REREMHfuXP744w+aNWtG5cqVef/992natKn+b+DixYtcvnyZP/74g19//ZV58+Zx4MABXnjhheLJlCKEKDFz585VHB0d9a+nTp2qODg4KIsWLVJOnz6tfPjhh4q5ubly9uxZRVEUJTw8XAGUQ4cOKYmJiUrXrl2VOnXqKFFRUYqiKMq2bdsUBwcHZd68ecqFCxeU9evXKxUrVlTGjRunvwag+Pj4KAsXLlTOnTunDBkyRLGzs1Pu3LmjKIqiDBo0SKldu7ayf/9+JTw8XNmwYYOyatWqbPN//fp1xczMTJk6daoSHh6uHD16VJk5c6YSGxurKIqizJ8/X/Hy8lKWLl2qXLx4UVm6dKni4uKizJs3T1EURUlOTlaqV6+u9OvXTzl69Khy8uRJ5ZVXXlGqVaumJCUlKYqiKL1791YcHByUt956Szl16pTy119/KTY2NsqcOXOK9sMQTxxAWb58uf716tWrFUCxtbU1+DEzM1O6d++uKIqiDBgwQAGUM2fO6I87cOCAAiinT58u+jwW+RmFEPmWOUh7e3srn332mUGaBg0aKAMHDlQUJT1Ib9++XWndurXStGlTJTo6Wp+2devWyueff25w/G+//aZ4eXnpXwPKxx9/rH8dFxenAMqaNWsURVGUTp06KX379s1X/tO+nC5dupTt/sqVKysLFy402PbJJ58ojRs31uetWrVqik6n0+9PSkpSrK2tlXXr1imKogZpPz8/JTU1VZ/mxRdfVF566aV85VGInGQO0osXL1ZMTU2V06dPK+fOnTP4uXHjhqIoijJmzBjFzMzM4DwJCQkKoKxfv77I8yht0kIYiZiYGK5fv05oaKjB9tDQUI4cOWKw7eWXX8bHx4d///0Xa2tr/fYjR46wc+dOg+EgWq2WxMREEhISsLGxAaBWrVr6/ba2tjg4OBAVFQXA22+/zfPPP8/Bgwdp06YNXbp0oUmTJtnmOSQkhNatWxMcHEzbtm1p06YNL7zwAs7OzsTHx3PhwgX69+/PgAED9Mekpqbi6Oioz+/58+ext7c3OG9iYiIXLlzQvw4KCsLU1FT/2svLi2PHjuXybgpRcHXq1EGr1RIVFUWzZs2yTRMaGkpqaioXLlygcuXKAJw9exYAPz+/Is+TBGkhSqFnn32W+fPns3v3bp5++mn99ri4OMaPH0+3bt2yHGNlZaX/3dzc3GCfRqNBp9MBau/Xy5cv888//7BhwwZat27NoEGDmDJlSpZzmpqasmHDBnbt2sX69ev59ttvGTVqFHv37tU/EPzwww80atQoy3Fp+a1Xrx4LFizIcm43N7d85VeIgoiLi+P8+fP61+Hh4Rw+fBgXFxeqVq1Kz549ee211/jqq6+oU6cOt27dYtOmTdSqVYsOHToQFhZG3bp16devH9OmTUOn0zFo0CCeeeYZqlatWvQZLvKyuRAi3/Jb3T1o0CBFUQzbpKdPn67Y2toqW7Zs0adt0qSJ0q9fv1yvSaYqPkVRFEdHR2Xu3LnZpp89e7Zib2+fr/tJTU1Vypcvr3z11Vf6+5kwYUKO6efMmaM4Ozsr9+/fzzFN7969lc6dOxtsGzp0qNKiRYt85UmIjDZv3qwAWX569+6tKIraT2LMmDFKxYoVFXNzc8XLy0vp2rWrcvToUf05rl27pnTr1k2xs7NTPDw8lD59+uj7dBQ1KUkLYUQ++OADxo4dS+XKlalduzZz587l8OHD2ZY033nnHbRaLR07dmTNmjU0bdqUMWPG0LFjR3x9fXnhhRcwMTHhyJEjHD9+nE8//TRfeRgzZgz16tUjKCiIpKQkVq9eTfXq1bNNu3fvXjZt2kSbNm1wd3dn79693Lp1S59+/PjxDBkyBEdHR9q1a0dSUhL//fcf9+7dY/jw4fTs2ZMvv/ySzp07M2HCBHx8fLh8+TLLli3jww8/xMfHp/BvphDZaNmyJYqi5Ljf3Nyc8ePHM378+BzTeHt7s3Tp0uLIXhYSpIUwIkOGDOH+/fu89957REVFUaNGDVatWkVAQEC26YcNG4ZOp+PZZ59l7dq1tG3bltWrVzNhwgQmT56Mubk5gYGBvP766/nOg4WFBSNHjuTSpUtYW1vTrFkzFi9enG1aBwcHtm3bxrRp04iJicHPz4+vvvpKP1nE66+/jo2NDV9++SUffPABtra2BAcHM2zYMABsbGzYtm0bI0aMoFu3bsTGxlK+fHlat26Ng4NDwd48IcogjZLbI4UQQgghSoxMZiKEEEIYKQnSQgghhJGSIC2EEEIYKQnSQgghhJGSIC2EEEIYqSc6SM+cOZOKFStiZWVFo0aN2LdvX67p//jjDwIDA7GysiI4OJh//vnHYL+iKIwZMwYvLy+sra0JCwvj3LlzxXkLuSrI/f3www80a9YMZ2dnnJ2dCQsLy5K+T58+aDQag5927doV921kqyD3Nm/evCz5zjj7FpTuz65ly5ZZ7k+j0dChQwd9GmP57LZt20anTp3w9vZGo9GwYsWKPI/ZsmULdevWxdLSkipVqjBv3rwsaQr6t1wcCnpvy5Yt45lnnsHNzQ0HBwcaN27MunXrDNKMGzcuy+cWGBhYjHeRs4Le35YtW7L9fxkZGWmQzhg+Oyj4/WX3N6XRaAgKCtKnKYrP74kN0r///jvDhw9n7NixHDx4kJCQENq2baufvzizXbt28fLLL9O/f38OHTpEly5d6NKlC8ePH9en+eKLL5g+fTqzZ89m79692Nra0rZtWxITEx/XbekV9P62bNnCyy+/zObNm9m9ezcVKlSgTZs2XLt2zSBdu3btuHHjhv5n0aJFj+N2DBT03kAdz5sx35cvXzbYX5o/u2XLlhnc2/HjxzE1NeXFF180SGcMn118fDwhISHMnDkzX+nDw8Pp0KEDrVq14vDhwwwbNozXX3/dIJgV5v9DcSjovW3bto1nnnmGf/75hwMHDtCqVSs6derEoUOHDNIFBQUZfG47duwojuznqaD3l+bMmTMG+Xd3d9fvM5bPDgp+f998843BfV25cgUXF5csf3eP/PkVyzxmpUDDhg31Uy0qiqJotVrF29tbmThxYrbpu3fvrnTo0MFgW6NGjZQ333xTURRF0el0iqenp/Lll1/q90dHRyuWlpbKokWLiuEOclfQ+8ssNTVVsbe3V3755Rf9tuymZywJBb23zFNvZlbWPruvv/5asbe3V+Li4vTbjOWzy4hspifN7MMPP1SCgoIMtr300ktK27Zt9a8f9f0qDvm5t+zUqFFDGT9+vP712LFjlZCQkKLLWBHJz/2lTb957969HNMY42enKIX7/JYvX65oNBqDFeGK4vN7IkvSycnJHDhwgLCwMP02ExMTwsLC2L17d7bH7N692yA9QNu2bfXpw8PDiYyMNEjj6OhIo0aNcjxncSnM/WWWkJBASkoKLi4uBtu3bNmCu7s71apV4+233+bOnTtFmve8FPbe4uLi8PPzo0KFCnTu3JkTJ07o95W1z+6nn36iR48e2NraGmwv6c+uMPL6uyuK98tY6HQ6YmNjs/zNnTt3Dm9vb/z9/enZsycREREllMPCqV27Nl5eXjzzzDPs3LlTv70sfXag/t2FhYVlWQnrUT+/JzJI3759G61Wi4eHh8F2Dw+PLO0laSIjI3NNn/ZvQc5ZXApzf5mNGDECb29vgz+gdu3a8euvv7Jp0yYmT57M1q1bad++PVqttkjzn5vC3Fu1atX4+eefWblyJfPnz0en09GkSROuXr0KlK3Pbt++fRw/fjzLNKDG8NkVRk5/dzExMTx48KBI/q8biylTphAXF0f37t312xo1asS8efNYu3Yts2bNIjw8nGbNmhEbG1uCOc0fLy8vZs+ezdKlS1m6dCkVKlSgZcuWHDx4ECia7yljcf36ddasWZPl764oPj+Zu1tkMWnSJBYvXsyWLVsMOlj16NFD/3twcDC1atWicuXKbNmyhdatW5dEVvOlcePGNG7cWP+6SZMmVK9ene+//55PPvmkBHNW9H766SeCg4Np2LChwfbS+tk9KRYuXMj48eNZuXKlQZtt2hzooK4B3qhRI/z8/FiyZAn9+/cviazmW7Vq1ahWrZr+dZMmTbhw4QJff/01v/32WwnmrOj98ssvODk50aVLF4PtRfH5PZEl6XLlymFqasrNmzcNtt+8eRNPT89sj/H09Mw1fdq/BTlncSnM/aWZMmUKkyZNYv369dSqVSvXtP7+/pQrV85gbdbi9ij3lsbc3Jw6dero811WPrv4+HgWL16crz/+kvjsCiOnvzsHBwesra2L5P9DSVu8eDGvv/46S5YsyVK1n5mTkxNVq1Y1+s8tJw0bNtTnvSx8dqCODPn555/p1asXFhYWuaYtzOf3RAZpCwsL6tWrx6ZNm/TbdDodmzZtMihxZdS4cWOD9AAbNmzQp69UqRKenp4GaWJiYti7d2+O5ywuhbk/UHs4f/LJJ6xdu5b69evneZ2rV69y584dvLy8iiTf+VHYe8tIq9Vy7Ngxfb7LwmcH6hDBpKQkXn311TyvUxKfXWHk9XdXFP8fStKiRYvo27cvixYtMhgyl5O4uDguXLhg9J9bTg4fPqzPe2n/7NJs3bqV8+fP5+vhuFCf3yN1OyvFFi9erFhaWirz5s1TTp48qbzxxhuKk5OTEhkZqSiKovTq1Uv56KOP9Ol37typmJmZKVOmTFFOnTqljB07VjE3N1eOHTumTzNp0iTFyclJWblypXL06FGlc+fOSqVKlZQHDx4Y/f1NmjRJsbCwUP7880/lxo0b+p/Y2FhFURQlNjZWef/995Xdu3cr4eHhysaNG5W6desqAQEBSmJiolHf2/jx45V169YpFy5cUA4cOKD06NFDsbKyUk6cOGFw/6X1s0vTtGlT5aWXXsqy3Zg+u9jYWOXQoUPKoUOHFECZOnWqcujQIeXy5cuKoijKRx99pPTq1Uuf/uLFi4qNjY3ywQcfKKdOnVJmzpypmJqaKmvXrtWnyev9MtZ7W7BggWJmZqbMnDnT4G8uOjpan+a9995TtmzZooSHhys7d+5UwsLClHLlyilRUVGP9d4Kc39ff/21smLFCuXcuXPKsWPHlKFDhyomJibKxo0b9WmM5bMrzP2lefXVV5VGjRple86i+Pye2CCtKIry7bffKr6+voqFhYXSsGFDZc+ePfp9LVq0UHr37m2QfsmSJUrVqlUVCwsLJSgoSPn7778N9ut0OmX06NGKh4eHYmlpqbRu3Vo5c+bM47iVbBXk/vz8/BQgy8/YsWMVRVGUhIQEpU2bNoqbm5tibm6u+Pn5KQMGDCiRPyZFKdi9DRs2TJ/Ww8NDefbZZ5WDBw8anK80f3aKoiinT59WAGX9+vVZzmVMn13asJzMP2n307t3b6VFixZZjqldu7ZiYWGh+Pv7K3Pnzs1y3tzer8eloPfWokWLXNMrijrczMvLS7GwsFDKly+vvPTSS8r58+cf7409VND7mzx5slK5cmXFyspKcXFxUVq2bKn8+++/Wc5rDJ+dohTu/2Z0dLRibW2tzJkzJ9tzFsXnJ+tJCyGEEEbqiWyTFkIIIUoDCdJCCCGEkZIgLYQQQhgpCdJCCCGEkZIgLYQQQhgpCdJCCCGEkZIgLYQQQhgpCdIFkJSUxLhx40hKSirprBS5snxvULbvryzfG5Tt+yvL9wZl+/4e173JZCYFEBMTg6OjI/fv38fBwaGks1OkyvK9Qdm+v7J8b1C2768s3xuU7ft7XPcmJWkhhBDCSEmQFkIIIYyUWUlnoLilpqZy6NAhPDw8MDF5tGeS2NhYAK5du0ZMTExRZM9olOV7g7J9f2X53qBs319Zvjco2/eX3b3pdDpu3rxJnTp1MDMrmvBa5tuk9+/fT8OGDUs6G0IIIZ4Q+/bto0GDBkVyrjJfkvbw8ADUN620LpQuhBDC+N24cYOGDRvq405RKPNBOq2K28vLCx8fnxLOjRBCiLLuUZtWDc5VZGcSQgghRJEq0SC9bds2OnXqhLe3NxqNhhUrVhjsVxSFMWPG4OXlhbW1NWFhYZw7d65kMiuEEEI8ZiUapOPj4wkJCWHmzJnZ7v/iiy+YPn06s2fPZu/evdja2tK2bVsSExMfc06FEEKIx69E26Tbt29P+/bts92nKArTpk3j448/pnPnzgD8+uuveHh4sGLFCnr06PE4syqEKMW0Wi0pKSklnQ1Rypmbm2NqavpYr2m0HcfCw8OJjIwkLCxMv83R0ZFGjRqxe/fuHIN0UlKSwVyqaWPZHtWGkzf5cftFGlZy4b021YrknEKI4qUoCpGRkURHR5d0VkQZ4eTkhKenJxqN5rFcz2iDdGRkJECWruweHh76fdmZOHEi48ePL/L83IpNYm/4XRyszYv83EKI4pEWoN3d3bGxsXlsX6yi7FEUhYSEBKKiogAe25Beow3ShTVy5EiGDx+uf33t2jVq1KjxyOd1jT/L26arsIupAtR/5PMJIYqXVqvVB2hXV9eSzo4oA6ytrQGIiorC3d39sVR9G+0QLE9PTwBu3rxpsP3mzZv6fdmxtLTEwcFB/2Nvb18k+XGLOckI88U0idtQJOcTQhSvtDZoGxubEs6JKEvS/j89rj4ORhukK1WqhKenJ5s2bdJvi4mJYe/evTRu3Pix50djqlZzmyjS+USI0kSquEVRetz/n0o0SMfFxXH48GEOHz4MqJ3FDh8+TEREBBqNhmHDhvHpp5+yatUqjh07xmuvvYa3tzddunR57HnVmKotA6aK9rFfWwghHlXFihWZNm1avtNv2bIFjUZT7J3u5s2bh5OTU7FeozQr0Tbp//77j1atWulfp7Ul9+7dm3nz5vHhhx8SHx/PG2+8QXR0NE2bNmXt2rVYWVk99rxqTC3Uf5XUx35tIcSTI6+S2tixYxk3blyBz7t//35sbW3znb5JkybcuHEDR0fHAl9LFJ0SDdItW7Ykt0W4NBoNEyZMYMKECY8xV9kzMVOru00lSAshitGNGzf0v//++++MGTOGM2fO6LfZ2dnpf1cUBa1Wm69lEd3c3AqUDwsLi1z7/4jHw2jbpI2Nib5NWqq7hRDFx9PTU//j6OiIRqPRvz59+jT29vasWbOGevXqYWlpyY4dO7hw4QKdO3fGw8MDOzs7GjRowMaNGw3Om7m6W6PR8OOPP9K1a1dsbGwICAhg1apV+v2Zq7vTqqXXrVtH9erVsbOzo127dgYPFampqQwZMgQnJydcXV0ZMWIEvXv3LnAT5axZs6hcuTIWFhZUq1aN3377Tb9PURTGjRuHr68vlpaWeHt7M2TIEP3+7777joCAAKysrPDw8OCFF14o0LWNjQTpfJKStBDCWHz00UdMmjSJU6dOUatWLeLi4nj22WfZtGkThw4dol27dnTq1ImIiIhczzN+/Hi6d+/O0aNHefbZZ+nZsyd3797NMX1CQgJTpkzht99+Y9u2bURERPD+++/r90+ePJkFCxYwd+5cdu7cSUxMTJY1GfKyfPlyhg4dynvvvcfx48d588036du3L5s3bwZg6dKlfP3113z//fecO3eOFStWEBwcDKhNqEOGDGHChAmcOXOGtWvX0rx58wJd39iUuXHSxcXETG2TliAtROmlKAoPUkqmNsza3LTIegZPmDCBZ555Rv/axcWFkJAQ/etPPvmE5cuXs2rVKgYPHpzjefr06cPLL78MwOeff8706dPZt28f7dq1yzZ9SkoKs2fPpnLlygAMHjzYoDny22+/ZeTIkXTt2hWAGTNm8M8//xTo3qZMmUKfPn0YOHAgoPZV2rNnD1OmTKFVq1ZERETg6elJWFgY5ubm+Pr60rBhQwAiIiKwtbWlY8eO2Nvb4+fnR506dQp0fWMjQTqf9CVppLpbiNLqQYqWGmPWlci1T05oi41F0Xzl1q9vOKFSXFwc48aN4++//+bGjRukpqby4MGDPEvStWrV0v9ua2uLg4ODfkat7NjY2OgDNKizbqWlv3//Pjdv3tQHTABTU1Pq1auHTqfL972dOnWKN954w2BbaGgo33zzDQAvvvgi06ZNw9/fn3bt2vHss8/SqVMnzMzMeOaZZ/Dz89Pva9eunb46v7SS6u58MjGVkrQQwjhk7qX9/vvvs3z5cj7//HO2b9/O4cOHCQ4OJjk5OdfzmJsbTnOs0WhyDajZpc+t829xqFChAmfOnOG7777D2tqagQMH0rx5c1JSUrC3t+fgwYMsWrQILy8vxowZQ0hISKmeu11K0vlk+rAkbSYlaSFKLWtzU05OaFti1y4uO3fupE+fPvpq5ri4OC5dulRs18uOo6MjHh4e7N+/X98OrNVqOXjwILVr1873eapXr87OnTvp3bu3ftvOnTsNpne2tramU6dOdOrUiUGDBhEYGMixY8eoW7cuZmZmhIWFERYWxtixY3FycuLff/+lW7duRXavj5ME6XwyNZfqbiFKO41GU2RVzsYkICCAZcuW0alTJzQaDaNHjy5QFXNReeedd5g4cSJVqlQhMDCQb7/9lnv37hWoLf6DDz6ge/fu1KlTh7CwMP766y+WLVum760+b948tFotjRo1wsbGhvnz52NtbY2fnx+rV6/m4sWLNG/eHGdnZ/755x90Oh3VqpXelQvL3v/WYmJqYcsVnRvRGntkqn4hhDGZOnUq/fr1o0mTJpQrV44RI0YQExPz2PMxYsQIIiMjee211zA1NeWNN96gbdu2BVqIokuXLnzzzTdMmTKFoUOHUqlSJebOnUvLli0BdanISZMmMXz4cLRaLcHBwfz111+4urri5OTEsmXLGDduHImJiQQEBLBo0SKCgoKK6Y6Ln0Z53A0Kj9nVq1epUKECV65cwcfHp9DnuR79gCaT/sXC1ISzn7UvwhwKIYpDYmIi4eHhVKpUqURmKRSg0+moXr063bt355NPPinp7BSJ3P5fFVW8yUhK0vlkZqJW16SWQBWSEEKUBpcvX2b9+vW0aNGCpKQkZsyYQXh4OK+88kpJZ63Ukt7d+WRmqr5VOgV0ujJd+SCEEIViYmLCvHnzaNCgAaGhoRw7doyNGzdSvXr1ks5aqSUl6XwyS4nhL4v/YYaOFG0bLE3M8z5ICCGeIBUqVGDnzp0lnY0yRYJ0PpmbmBBscgmA+JQULM0lSAshhCheEqTzyczajj7JH5KCKd/ppJVACCFE8ZMgnU9mZuZs0dUGIIWimX9XCCGEyI0UCfNJo9Fg+rCHt1Y6jgkhhHgMpCRdAC+absVEk0Lqg4bgIOMuhRBCFC8J0gUw3uQnLE1TuBL/JuBW0tkRQghRxkl1dwGkatSp7XTapBLOiRBC5K5ly5YMGzZM/7pixYpMmzYt12M0Gg0rVqx45GsX1XlyM27cuAIt3FFaSZAugNSHFQ+pKSklnBMhRFnVqVMn2rVrl+2+7du3o9FoOHr0aIHPu3///izrND+qnALljRs3aN9epk8uChKkC0DLw5K0BGkhRDHp378/GzZs4OrVq1n2zZ07l/r161OrVq0Cn9fNzQ0bG5uiyGKePD09sbS0fCzXKuskSBeAviStlSAthCgeHTt2xM3NjXnz5hlsj4uL448//qB///7cuXOHl19+mfLly2NjY0NwcDCLFi3K9byZq7vPnTtH8+bNsbKyokaNGmzYsCHLMSNGjKBq1arY2Njg7+/P6NGjSXlYSJk3bx7jx4/nyJEjaDQaNBqNPs+Zq7uPHTvG008/jbW1Na6urrzxxhvExcXp9/fp04cuXbowZcoUvLy8cHV1ZdCgQfpr5YdOp2PChAn4+PhgaWlJ7dq1Wbt2rX5/cnIygwcPxsvLCysrK/z8/Jg4cSIAiqIwbtw4fH19sbS0xNvbmyFDhuT72sVJOo4VgFZjCgoo2uSSzooQ4lEkxxf8GFNLMH34lalNBW0SaEzA3Drv81rY5vsyZmZmvPbaa8ybN49Ro0bp12L+448/0Gq1vPzyy8TFxVGvXj1GjBiBg4MDf//9N7169aJy5co0bNgwz2vodDq6deuGh4cHe/fu5f79+wbt12ns7e2ZN28e3t7eHDt2jAEDBmBvb8+HH37ISy+9xPHjx1m7dq1+rWdHR8cs54iPj6dt27Y0btyY/fv3ExUVxeuvv87gwYMNHkQ2b96Ml5cXmzdv5vz587z00kvUrl2bAQMG5Ot9++abb/jqq6/4/vvvqVOnDj///DPPPfccJ06cICAggOnTp7Nq1SqWLFmCr68vV65c4cqVKwAsXbqUr7/+msWLFxMUFERkZCRHjhzJ13WLmwTpAtBqzECRNmkhSr3PvQt+zIvzIKir+vvpv+CPPuDXFPr+nZ5mWjAk3Ml67Lj7BbpUv379+PLLL9m6dat+HeW5c+fy/PPP4+joiKOjI++//74+/TvvvMO6detYsmRJvoL0xo0bOX36NOvWrcPbW30vPv/88yztyB9//LH+94oVK/L++++zePFiPvzwQ6ytrbGzs8PMzAxPT88cr7Vw4UISExP59ddfsbVVH1ZmzJhBp06dmDx5Mh4eHgA4OzszY8YMTE1NCQwMpEOHDmzatCnfQXrKlCmMGDGCHj16ADB58mQ2b97MtGnTmDlzJhEREQQEBNC0aVM0Gg1+fn76YyMiIvD09CQsLAxzc3N8fX3z9T4+DlLdXQA6jfpMk5IsvbuFEMUnMDCQJk2a8PPPPwNw/vx5tm/fTv/+/QHQarV88sknBAcH4+Ligp2dHevWrSMiIiJf5z916hQVKlTQB2iAxo0bZ0n3+++/ExoaiqenJ3Z2dnz88cf5vkbGa4WEhOgDNEBoaCg6nY4zZ87otwUFBWFqaqp/7eXlRVRUVL6uERMTw/Xr1wkNDTXYHhoayqlTpwC1Sv3w4cNUq1aNIUOGsH79en26F198kQcPHuDv78+AAQNYvnw5qampBbrP4iIl6QLQmliAFpKTHpR0VoQQj+J/1wt+jGmGjlCBndRzaDKVc4Yde7R8ZdC/f3/eeecdZs6cydy5c6lcuTItWrQA4Msvv+Sbb75h2rRpBAcHY2try7Bhw0hOLrqmuN27d9OzZ0/Gjx9P27ZtcXR0ZPHixXz11VdFdo2MzDMtWqTRaNDpdEV2/rp16xIeHs6aNWvYuHEj3bt3JywsjD///JMKFSpw5swZNm7cyIYNGxg4cKC+JiNzvh43KUkXgNbEAoAUCdJClG4WtgX/Mc1QpjE1U7dlbI/O7byF0L17d0xMTFi4cCG//vor/fr107dP79y5k86dO/Pqq68SEhKCv78/Z8+ezfe5q1evzpUrV7hx44Z+2549ewzS7Nq1Cz8/P0aNGkX9+vUJCAjg8uXLhrdrYYFWq83zWkeOHCE+Pr29fufOnZiYmFCtWrV85zk3Dg4OeHt7Z1kmc+fOndSoUcMg3UsvvcQPP/zA77//ztKlS7l79y4A1tbWdOrUienTp7NlyxZ2797NsWNF99BVWFKSLgDdwyfpVAnSQohiZmdnx0svvcTIkSOJiYmhT58++n0BAQH8+eef7Nq1C2dnZ6ZOncrNmzcNAlJuwsLCqFq1Kr179+bLL78kJiaGUaNGGaQJCAggIiKCxYsX06BBA/7++2+WL19ukKZixYqEh4dz+PBhfHx8sLe3zzL0qmfPnowdO5bevXszbtw4bt26xTvvvEOvXr307dFF4YMPPmDs2LFUrlyZ2rVrM3fuXA4fPsyCBQsAmDp1Kl5eXtSpUwcTExP++OMPPD09cXJyYt68eWi1Who1aoSNjQ3z58/H2traoN26pEhJugCUtCCdnFjCORFCPAn69+/PvXv3aNu2rUH78ccff0zdunVp27YtLVu2xNPTky5duuT7vCYmJixfvpwHDx7QsGFDXn/9dT777DODNM899xzvvvsugwcPpnbt2uzatYvRo0cbpHn++edp164drVq1ws3NLdthYDY2Nqxbt467d+/SoEEDXnjhBVq3bs2MGTMK9mbkYciQIQwfPpz33nuP4OBg1q5dy6pVqwgICADUnupffPEF9evXp0GDBly6dIl//vkHExMTnJyc+OGHHwgNDaVWrVps3LiRv/76C1dX1yLNY2FoFEUx2iWdtFot48aNY/78+URGRuLt7U2fPn34+OOP9dU+ebl69SoVKlTgypUr+Pj4PFJ+Tkx/ngp3drHN/z069n4/7wOEECUmMTGR8PBwKlWqhJWVLIgjikZu/6+KMt6kMerq7smTJzNr1ix++eUXgoKC+O+//+jbty+Ojo4lMtB8ffWJfLPpHK86+tLxsV9dCCHEk8aog/SuXbvo3LkzHTp0ANT2j0WLFrFv374SyY+tpTo8ICEp944SQgghRFEw6jbpJk2asGnTJn2vxSNHjrBjx44Sm7jdxkJ9polLMo7xc0IIIco2oy5Jf/TRR8TExBAYGIipqSlarZbPPvuMnj175nhMUlISSUnpk43ExsYWWX6CbiznN/M/OHu3NVC/yM4rhBBCZMeoS9JLlixhwYIFLFy4kIMHD/LLL78wZcoUfvnllxyPmThxon7aPEdHx3wPScgPp8SrNDM9jlvS5bwTCyGEEI/IqEvSH3zwAR999JF+Ltbg4GAuX77MxIkT6d27d7bHjBw5kuHDh+tfX7t2rcgC9X3/jgw9Zk6KXVWeK5IzCiGKmxEPYBGl0OP+/2TUQTohIQETE8PCvqmpaa5TxVlaWhoMpo+JiSmy/CheIazUxeOjtc47sRCiRKVN55iQkIC1tfzNiqKRkJAAZJ3GtLgYdZDu1KkTn332Gb6+vgQFBXHo0CGmTp1Kv379SiQ/tpbq25WQLL27hTB2pqamODk56RdpsLGxyff8CkJkpigKCQkJREVF4eTkZLAYSHEy6iD97bffMnr0aAYOHEhUVBTe3t68+eabjBkzpkTyY594gzYm+4lNcgKeKZE8CCHyL20JxfyupiREXpycnHJdmrOoGXWQtre3Z9q0aUybNq2kswKAw41dzLH4mk3aOqRqh2BmatT97oR44mk0Gry8vHB3dydF1oEXj8jc3PyxlaDTGHWQNjYWVmq7liXJ3IxNoryTtHMJURqYmpo+9i9XIYqCFAULwNziYZDWpBA66d8Szo0QQoiyToJ0QZipk6lbolabJaZIBzIhhBDFR4J0QVjYAGCLulTlrdik3FILIYQQj0SCdEHYqQuUu2miAYiSIC2EEKIYSZAuiIdB2kHzAGsSuRWbWMIZEkIIUZZJkC4IS3v9rx1N90h1txBCiGIlQbogMsxW9KnZXK5FJzJ1/Rn2XrxTgpkSQghRVkmQLiRLTQpuO8fx7+b1vDRnT0lnRwghRBkkQbqgWo5EpzHlhuJCf7M1hJqcKOkcCSGEKKNkxrGCavkRhyv0Yt5PM6ljcp7DuiolnSMhhBBllATpQvAu58oqXSirdKH6bVqdgqmJrLAjhBCi6Eh1dyG426evV11Lc4GvzWeSvHtOCeZICCFEWSRBuhBMTDR817MuALaaRLqa7sRq40iIl17eQgghio4E6UJq7O8KwH5dNQA0ihZ+fxV0Mp+3EEKIoiFBupAcrM0BSMWMUSn91I0Ru+DQ/BLMlRBCiLJEgnQhZewktkAbxrnqg9UX+38ooRwJIYQoayRIF5FVlh3BxAwij8HJlXDgF7hzoaSzJYQQohSTIP0INr3XgpfqVwDg9xPxEPKyumPJa/DXEHSzQtFdP1qCORRCCFGaSZB+BJXd7PiwndpxLCo2idQ2E6FiM/1+k9QHJP/YDu6Gl1QWhRBClGISpB+Rk42Fft2Ne6kW8Noq6L+B7nzBKZ0vVrp4SIot2UwKIYQolSRIPyJTEw1OD3t6H792n683neeArgonlYoMSRnMXl0geNUq4VwKIYQojWRa0CLgbGvBvYQU+s7bD8A3m85hZW7COcWHHskfEw6QmgSr34W6r4HvUyWaXyGEEKWDlKSLgIkm65zdWp0CgIIJKIo60cnhBXD9sJrg2kE4t+Ex5lIIIURpI0G6CFy+E59lW4pWSX+h0UCHqeAeBNpkdZuVIyx8Cf6bCykPHlNOhRBClCaFqu6+cuUKGo0GHx8fAPbt28fChQupUaMGb7zxRpFmsDSwszTjXkJK7omcKsBbO9KD9PFlYGELq4fB3u/hpfng6AObxsOtM+BdGy5shleXgo1Lcd+CEEIII1SokvQrr7zC5s2bAYiMjOSZZ55h3759jBo1igkTJhRpBkuDb1+uSzUPe5a+3ZgaXg45JzQxAXMr9fdGb0Ctl8DcBm6dggUvwJbPYc93cGETbP8Krh9U27JTk2DBi7DqnfRzKQqsGAiLe4I2Va0+v3uxeG9UCCHEY1WoIH38+HEaNmwIwJIlS6hZsya7du1iwYIFzJs3ryjzVyo0DSjHunebU8/PhXp+zln2v/T9br5af4bkVB3z91ym88ydXIg1ZabNW0S9thUs7OBeOOz8Rj3AzjP9YF0KnFgB59argVmnUwP4rCZqG/fp1bBnJvzYGn5qA0lxj+emhRCiuCiKWjgJ36Z+5+Xl7kVYOUhNX8YUqro7JSUFS0t1TeWNGzfy3HPPARAYGMiNGzeKLnel0JDWAfy257LBtr3hd9kbfhetTuG7LepUoa2/2grAv6edWdp0GPz7KaCB+n3h2SmweybYuYOTr1r9be8FTn5w+yzsnA6J0ekX2DBG/Tf+FhyYBxUawbYvoHJrqNcnvfSek6jT4FgeLO2L4i0QQoiCS06AtR9B5FGIvw33r6jbqz8HXWer204sB+864N9C3afTwd/vqt97AKf+ggGb4dJ2uLIPGg5Q0+fmzgVwrqTWdBqhQgXpoKAgZs+eTYcOHdiwYQOffPIJANevX8fV1bVIM1jauNlbMrBlZX0wzmjerktZth24fA/e/gAavK7O/Z0WKEOHpCdyrgSN3lJ/Yq9D8w/U/6zX/suaAdty4FFD/Y93bj1UaQ22brBlEiha8AsFnwZg4wp3zsHV/erQMJty0G8dlKtSRO+EEOKJpE0BU/N8pEtVS8DrP4bkODC1gIubs6ZLeQAm5hB7AzaOhZBXwLcxnPkblr4OutT0tIn34du66a9PrYa3toFzRfj7Pbh3GXr+oe5bNgDOb4QH96BWD+g8I3/5fswKFaQnT55M165d+fLLL+nduzchISEArFq1Sl8NXlSuXbvGiBEjWLNmDQkJCVSpUoW5c+dSv379Ir1OUXq/TTVeqOfD0w9Ly/2bVmLFoWvciU/ONv2DZC3W1lmryfXKVYGmw9TfXfyhyWD1J+Y6/PMBROyBXsvBLRDMLNT/+A/uQesx4FpFnfHs7Fq1Sn3fnOyvkXAbEu4AVdRFQpITwLdR4d8EIUTppyjq6JS4KLX/i507PPdtemfWpFi1Ju7gL2qB4MYR2PE1WDqo31m1eqi1dNn5+104+GvW7a3HgFt1MLNUS8Fp13KtAhWegojdcPR3WDU4/RjPYOj+m9pPJ2JX+nb36qDTqr+7+MP+H2Hbl1DzeTj9N6QkqPvuXlCDfVkJ0i1btuT27dvExMTg7JweXN544w1sbGyKLHP37t0jNDSUVq1asWbNGtzc3Dh37pzBNY2RiYkGfzc7GlR0Zv+le7z6lB8XbsWx5cytbNPXnrCedcOaU7GcbcEu5OANPRaoVT4Zq2pc/OHDi+jnK7VygA5fwZ991SfNzMxt4cMLYG6t/lFungj3I9SStUUB8ySEKH1SEtWmNEcf2PoF3DwOt8+BT3115ElKglqSvboPgl+EoC7qd8nXwZD08Dvl0G/p50uKgU0TYN8PMHAPWDsZXk+ny3665IrNoNl72efRthz0X/fweC1EX4ZdM8A9UC2kWDtD33/UBwWNRm0qtHRUvxt1OtCYqLWIwS+otZZ1eqn3bGKqfj+aWz/qu1gsNIqiKHknM/TgwQMURdEH5MuXL7N8+XKqV69O27ZtiyxzH330ETt37mT79u2FPsfVq1epUKECV65c0Q8Ze1wSklO5/yAFL0drpm44y/RN53JM+1pjPyZ0rgnAnbgk/j0dRdc65TEzLcJ2EkVRO1ZcOwBbJ0Nqorq97efQeJD6e2oSTK+rPhV3+kYtkUedVIeMndsAzd6FOq+BqRncPg/nN0DtnuqDgDYVjixS/4jzagcXQpQsnRbWj1a/E24eU7e5BcKt04bphp9SCwRXD0B8FFRrr1ZpL+4J59YZprV1V0ujHjXVduGUBGj2PrQenX0e7l9VHwxOrVab3kKHFmzIaXK8OkImmwmlSkJxxJtCBek2bdrQrVs33nrrLaKjowkMDMTc3Jzbt28zdepU3n777SLJXI0aNWjbti1Xr15l69atlC9fnoEDBzJgwIB8n6Mkg3RG16Mf8MzUrcQna7Pd/3rTSnzcsQYADT/bSFRsEp90DqJX44rciUti/6V7hFV3L7qgrdOqQfpBdNbqqKjTsGUiNBsOe+fA4fmG+8vXU9uwL25OH/ftWQv6b4Dvm0GNztDiI9g+RW1rajpc7cGecAccvNKr0IQQeVMU9UHZzkMtTeb3mNRENfjFXFODn0ajvr5zDhq/owbQpa9nDbSZDdyjVhtnPv+5DepDvHt1eHBX7djqUik9zam/YOkAtfTrFQK3zsLPbaHFCHjqrYK9B6VEccSbQlV3Hzx4kK+//hqAP//8Ew8PDw4dOsTSpUsZM2ZMkQXpixcvMmvWLIYPH87//vc/9u/fz5AhQ7CwsKB3797ZHpOUlERSUpL+dWyscaxA5e1kze9vNubnneEsO3gty/604BuXlEpUrJr/PeF36dW4Ij3m7OFcVBwfd6jO/QcpPBfiTYBHzj2xoxOSOXUjlqf8XdDkFAxNTNWq7Oyqs90Dofsv6u/N388apK8dMHxt7ax2uri4We1hfmkntHz4MLHrW/UHDaCobUEnVoBHkPoEbemgtqNbu6htUSamOd6XEKWKoqgPw6Z5fM3ev6rOQHjrrNoGm/Fv9tifsHe22km0+Yfw9Ci1mlibknuJc9ME2DE1/bVXiNpZ9PeeajVw03dh7QS4tAMqNVdr0K7sVdM6+aqB2cI2505gGg1UbZP7fVXrAI0HQvSV9Os/uAu7pkPDN4y2N7WxKVRJ2sbGhtOnT+Pr60v37t0JCgpi7NixXLlyhWrVqpGQkFAkmbOwsKB+/frs2pXeEWDIkCHs37+f3bt3Z3vMuHHjGD9+fJbtJV2STnMo4h5dv9uVZXuLqm642FpQxd2OL9ed0W/vWqc8yw8ZBvVK5WzZ/H7LHK/R8dvtHL8Wwzc9atO5dg6dNgpCm6J+MVg5wdz2cGWPuv3FX9SgGn9LrQLXJqtP5s99C7aucHmXmj431s7q0zjAK0ugatE1lwjxWOl0cGqlWst0eac6jNLSXv1/HXcTKj+tBrctk8GtmtquCzDJ17CviE05tSNnRt511ZoqUzPYPhVOroQ3txqmib+jdppKvK+OGc7o/fNqkJxRTz3XCz+pf9eKoj4kg9qWe/MkBDyT/xJ7Qeh0cGwJxEaqQ6PKYH8XoylJV6lShRUrVtC1a1fWrVvHu+++C0BUVBQODrnMuFVAXl5e1KhRw2Bb9erVWbp0aY7HjBw5kuHDh+tfX7t2Lcs5SlIVd7tst289m32nsswBGiD8djyrj17Hzc6SRv5Zh7wdvxYDwKJ9EUUTpE3N05/aX10Kp1ZBQFs1EBukM4OXF6a/rvCU2rvz6GI1wGcc2+1cUR1W8cJPcPA3tZdn5DF1ARI7N6jXN2uVeMQetVdn2Dj19Y0jaskjbdha/B21bTynHpqKola/y3jwJ0tSLJhaqsEo6tTDGpyH/wd0OoiLVNtcQQ1whxaonaRA7fHbemz++ljs+OrhfAcZrx0Ds0PV31uNUvtvhG9TR1kEtFH/JrSZphTOHKBrvqA2PZmaqcOR9s5WA25SnBro5j+vzlKYkxYfqX9TAK9vSv9bzvx34hWi/hQXExMI6VF85y+jChWkx4wZwyuvvMK7777L008/TePGjQFYv349derkMXC8AEJDQzlz5ozBtrNnz+Ln55fjMZaWlvqJVgBiYmKKLD9Fwd4q/Q/D0syEpNR8zKaTjcELDwEQPvHZHKu07z9IzXb7I7G0g9qv5C+tiQl0+179ATVIQtbg29wTnv5YHU52bIm6LWKvOqwC1CFl1do/nAI1CaqEwerh6pdrQFvouQTOrofFr6jt4S/8ZHj+5AT1vGfWwoV/1YkRanbLPs/aFLhzHlwD8q6mzIlOp07vam4NDfoX7hzG7up/arNH3d7pAezMGrUdMmycOlRHm6q2f9p5qDUmxd0P4co+tYQZ2EHtxXvtAKBRq3jjb6kz+cVFquNsu85S089/Xg2WHaaAmTUs7GE4hAfU/Dcdpv4/vHZQHc4Y0BZqvahONFSuqvrwt2dW7vlzraKWIm8chiZDwMJG/T/y/E/q+7PmA3V/7Z7qPAjRl9Vq4XaT06uGza3VCYpOrFAD9L45hgHazkNtQqrUXJ12OPE++LdM3y/rAJQ6haruBnXO7hs3bhASEoLJw/9A+/btw8HBgcDAwCLJ3P79+2nSpAnjx4+ne/fu7Nu3jwEDBjBnzhx69uyZr3MYS8exjGb8e44p68/yc5/6WJia8upPewt9rkOjn8HZ1sJgW8WP/gagvJM1Oz96+pHy+lhd3g3L31S/nHKiMVGr7g7PhwO/wOsb1fGUkyumd2JrOVL98qzfTx1Lfug3tRdohYZqKcShPAw7ro6N/K2r2oEm4Q74NYXLO9RzOJQH/1YPg7kCjhXUKsrkePizn/pl+exXam3CxS3qsDXPmhC+HW5neLB8a4f6sJHfCR6KS2qy2mvXIyj3dv8H0YbDZVIeqDUdgR3UoIIG1o6EIw9rTOq+Bp2mq+/9j63h5gl1VECdXvBHbzVoArT/Up2vPjcpD9QHrZjrag3JUwPBq1b+73FxT3WaXMi+l3Ian4bQZ7U6ecbCl9SOU8Hd1aB3YK7aJusaYBj8KjZTeysDVG2nNvXcOa+WkstVVT/n22dhz2y1VO7TQF03ftHLatCv8gwEPqvOnhUdoS6gk5lOp044ZGqe/86VsTdhxdvq5B4+9aBi88I/XIpHZjS9uzNnCii2ALh69WpGjhzJuXPnqFSpEsOHDy+VvbszUhSFmAepONqYE52QTO0JhV9XukFFZ5a82digNJ0WpO0szfj2lTqcuxnLgGb+OXciMzbnN8HJFWr19u2zhvv8W8JrD7/4UxLVUpw2BeZ1TG8rz8mL89T275BX1OMURQ0smTvCZSet085/P6sztLlUhkF71fGW451yPk5jCs5+6pd2txwmkgF1JiQzK7D3yLrvxlE4/qc6wUPMNfWBol4f9aEhI0VRJ6xx9E3/ok5Nhn3fqz1xw7eq1aTNP1QDiWtltXR37YD6JZ94H2Y0gErN1NIdwLI30ms3smNirpYyGwyAhS+qAbDV/+CXTupY28ws7KHNJ+r0t4rysK/DwyaybV9mrS528Vffu9QkdYiOk6/aU7la+/R7Tvt/rU2FT3KY8bB+f7W06t9SnXUvrWQac0PttGVho3byunVGfV/MLNX/V9NqqbP8ZeQVAr1WqNNU/hSmjnbotTz76ybGqBOByEx+TwSjaZPW6XR8+umnfPXVV8TFqQs62Nvb89577zFq1Ch9yboodOzYkY4dOxbZ+YyBRqPB0UYtVTnZWHD6k3a8/st/7Dh/O0vaT7vUpJydBW/NP5jtufZfusfVew+4ci+BOdsu8mHb9FqMuKRU+s7dD0D9ii7U9c06CcyFW3HcjU+mQUUjqgar0lr90elg6yR13nJQO+O0+Sw9XVo1q6k5BHWFViPVZT/P/JO1842Dj1oCyjhhgTZFbe9LuAP3LqkBxquWWoq+flCtugV1WtW0QFK3t5q+XDX1ugczTOCQUf3+8N9PaoC+exGq5tCBbvdM2P0dxFxVh7AMPaJuv3VaDR6xkfBDq6zHHV+mBqrYG2p+us1RHwhmN1eDUYcpaie8nd/A5gyBL+EOrB2RzXv+jDppRfALalV9wzfV4JhWO5Fd+gb91ffazkN9uHjz4eIGMdezD9AAybFqp6S0UvPlXfDeaTWYpQVo3ybpVc53Lxqu7hZzTQ3Wb+8Gt6owv5t6rhd+Vh88uv0Ay99Sq9t7rVCrlt1r5Fwid/BK/93EVJ1SN42pOXSZqX7Gt8+pTT3Pfav2p0jrp/HKErWWJSdWDun/d4QohEIF6VGjRvHTTz8xadIkQkPVThE7duxg3LhxJCYm8tlnn+VxBpGRlbkp819vxMtz9rD74h0AJnQOwsnGgudCvLl4K/eVrS7ciqPPw2Cs1WVfMXInLvsv27SFPrZ/2IoKLkU3W1yRMDFRS2Vp6vfNOW3auMuKzdXSpJ272l7n11T9UjWzzDqjkJmFOkSk0ZtqG6tPfcOqYG2K2taYccpWE1N17vQ0NZ5Tq0WdK6rBxTNYDVI+9dRjHcqrnYNafKimjzoN8zqowUCnS69eB7WaP+6mGrh3TYcOU9M7MGUWF6lWzYL6EOFUUX2/Wn4E60epHfVAvaeMKjZTr5G5hkLRqQ891Z9T71GXqj4oXM/wcGhqCS1HqM0A5R/Oj1wtm4cPB29o/4X6kPP8T+r93nk4kU9QV6jUQp2S8cK/6rmsndX8WjtDjS7Q8Wu1U+D1Q+pEFdcOgL2nmq/tU9Ve0q5V4P41dc11lPT5m2t1Vx9OTMzV0rH7Iza9VX5a/cmJ71OPdn4h8lCo6m5vb29mz56tX/0qzcqVKxk4cCDXrmXtkVxSjLG6Oyc95uxmz8W7AFya1EG/XatTqPy/fx7p3F88X4vuDQyf+BNTtASOXgvAwgGNaFK5GIZdiHQRe9SOSsm5PHT1+Uetnj65Ep6boZZSj/+pBqWY6/DU22ppckF3dRlTWzf1IaPJEPVBJOUBnF2XPrwH1A5Sp/9Wt3nXUauItcmARu2pH3NdrT7PqcSn06VXKRemyST2pvpAUaOLYdCM2KP2UA4IU18nx+c9LCflgdovIe1e145UH4zKagc9UaoYTXX33bt3s+0cFhgYyN27dx85U0+qHArBmJpo2PJ+S37ZfYm5Oy8V6txztl+kmqc9weUdWXM8knp+zqRo03uWWzycTOV2XBLl7CxzOo14FBUaQZfv1LbnS9vhbrhaAm89Bhb1UEuWt89A91/VwGZbTi09Zp40wq0aDDmk1gxkHs9qbm0YoEHtMFchw8I3Go0a5ECt3s7LozZf2XuoJfzMMpdC8zNuNmNtiLk1dJr2SFkTwtgVKkiHhIQwY8YMpk+fbrB9xowZ1KpVgN6YwkB5p5wneK9YzpaxnYI4ePkeR65ms0hGHs5HxdF55k79aytzE2a8nL6kW0Kylt/3RzBi6TE+7lCd15v5F/gaIg8ajTpEDLJO2uIaoFaF372odpLKrgNZRk65tIMKIcqMQgXpL774gg4dOrBx40b9GOndu3dz5coV/vnn0apln2Qjnw3kbnwyPRv55pimd5OKDF+idi4yNdHk2Aadl8QUHQv3Rehfv/bzPv3vn/59Ktsgffzafb5Yd4aPO1Snai7TkopCaPiGWo2dmqy2pwohBFCoeqwWLVpw9uxZunbtSnR0NNHR0XTr1o0TJ07w22859HYVeXK3t+KXfg1pE+SZY5rGldOHmPhl6ugVUsGJXR89zaddatKgYt7Lef57OqpA+Ru44CDbzt7ilR8KP65b5KBcFXXZ0We/kDmNhRB6hR717u3tnaUX95EjR/jpp5+YMyeX8aDikXg6pE9PaGZq2InHydocbydrXn3Kj+iEZPZfUufE7ljLi9VHc+glXAARd9U52W/HJaHTKZiYlJJx10IIUUrJI3spk3FCEkdrw2rRIa0D9L9nHE7VN7QSQzPsq1neAdM8AmzGavQHyVpStTqDjr1X7qUvonI+Ko5Np24aHH8/IYX/Lt3lEefKEUKIJ5rMH1cKje5Yg1lbLjD+uZo8O12dqvDdsKrU80uv4vZxTg/SzjbmDG0dwAv1fLAwM8Hd3pJ1JyKZvyeCUzdiuBOfdQx1g8820q6mJ93qlKfHnD08F+JNxnh7+U4Cfq5qb9ywqepY62UDm+DhYMXXG87y5wF1Jrrve9WjbZAniqKUnhnPhBDCSEiQLoX6N61Ev9CKhlOBljNsn/ZxTu8p7mBtjomJxqB03a6mF+1qejFs8SFWHM407SFwNz6ZhXsjWLhX7Vy2LNNqXJfuxNMcN4OS8tEr0Xyx7gwJyVr9trXHI/nn2A0ORtxj7dDm2FrKfzkhhMivAn1jduuWw8pBD0VHRz9KXkQBpAXoL56vxZ6Ld3g22Mtgv7u9Jd3qlEerKLhmWoAjoxRt4aqjL91O4PCVaIPOZzGJqQYBGtTx17//dwWAjaduPtLSmf8cu8Gvuy8x7aU6eDrmY+lAIYQo5QoUpB0dHfPc/9prrz1ShkTBdG9QIctMYqAG8akv1c7z+IwTmhTEuhOR/Lwz3GDb1A1nc0itSk7VsffiHd5ecJDPu9akXU2vXNNnNnCBOkXlF2tP5+vehBCitCtQkJ47d25x5UOUkIIG6XJ2FtyOS+Za9IN8pY9PTl/TOlmro9fP+0hO1fHW/IMGU5+COk3pmchYavk4EpuUSuT9RCLvJ+JkY05w+fQHxEt34guUZyGEKK2kgfAJZ22Ry9rC2XCyUYN0fsUmpgfpe/HJJKdm/1CQotXx0pw9HLkSzeiONfhx+0Vu3E/U7989Mn2Rg9RCTuAihBCljQzBesKNbF+dAHc7JnUL5siYNlT1sMs23csNfanibsfYTjWy3Z+TmzHpgXbKesPq8NlbLwDw7+mbDFt8mCNXogH4ZPVJgwANcOpGjP73K3cTDH6fuOYUC/dGcP9BSoHyJoQQxk5K0k+4Ci42bBjeQv/axiL7/xLvtalKOTtL4pLSS8avN61EpxBv3B0sOR8Vh5W5KWdvxjJqefpawtdzqRaftOY0bzb3p9+8//LM596L6Qu33H+Qoh/SNXjhQf1c5pvPRPHDa/VzOoUQQpQ6EqSFgf89W53u3++mf9NKdK1TntjEVOwszfQrY9lmqB63sTQjpIITAF6O6pCvBhVdDIJ0TIbq7jS1Kzhx+GGp+WDEvXzlK22dbVBXC4tLSmXuzksGi41sOHkzu0OFEKLUkiAtDDSs5MLhMc/gaG2e7eQjGbf5l8t+acFf+zU0WLAjs1mv1uXDP4+y/dztfJWiAY5mWvnri7Vn+G3P5XwdWxinI2N4kKyljm/ec6ALIURxkTZpkYWTjUWus4PN6VWPN5r70ynEO9v9zau68cULhkuWLhzQCDtLM0KruOLlaK2fHa2w7ci/77+S7fbZWy+w4eRN7iek8POOcIYuPmRQRZ+dxBQtG0/eJOFhT/TkVB3tpm2n63e7uB2XVKj8CSFEUZCStCiwNkGeua7UBeBglT6v+G/9G9Kkcjl2jngaS3P1ufApf1fgXIGvbW9pRmxSKsk5DB2btOY0AG72ltyKVQPsysPX+aBtNQa1qsKqI9eZtvEsM1+pS3UvBwD+t+wYyw5do3Ntb77pUcegCv7ynQTK2Vmi1SlZ5ju/cf8BN+4nUldK20KIYiIlaVEsMg7tSmuvdrQxx8pc3d6okgujOxr2FB//XBD1M8w/nnEucgBrc1PcHSzzdf20AJ3my3Vn6P3zPoYsOsTFW/F8sVYN5t9tOa+f8nTl4evM2nKBHnP26I+7cf8Bq49ep+bYdWzM1ObdeOK/dPtuFyeuG1bFCyFEUZEgLYqFVpde0vV2yjqFp0ajoX/TSrSq5gaoPcV7N6mIk036FKaTugVT+2HHNAAbC9M8V+/Kzdazt/S/p421/mLtGYM0kx8G7zTXox8weOEhHqRoef3X7NvPf9oRLqt9CSGKhVR3i2JhaZahF3gOw7oAZr1aj3+O3aB1dQ/AcFy1t5M1ugzB7058ssFymdbmpjxIUecKt7M0y7PtOaPt526zJId27YyuRydmuz1jUF528Brta3rxTA2PfF9fCCHyQ0rSolg09nelb2hFvnoxJNd0VuamdKvro18b28E6PaDbWppxPipO/7qCizV3MyyrWdfPSf97eaf0Vb/y68OlR/NMk3mI2OCFB0lM0ZKYYtgmvvzQ1QJfXwgh8iJBWhQLExMNYzsF8Xw9nwIdN/65mrSp4cH6d5sDUM3TXr/v+1frk3FG0Pp+LvrfXXJZ6etRZB76tfroDTacvElsomGv9Iw1BxntC7/Ll+tOF3ohEyHEk02qu4VRqeJux5wMs4ZNeTGEOVsvMrBVZfxcbanl48jRq/ep5+dMv9BKrDh8jSBvh0IvuVkY7yw6xB9vNTbYZmWe/ry749xt7KzM8HOxofv3uwGwMDVlaFiAwTFHrkRzNyGZVtXciz/TQohSSYK0MGqV3eyYnGHM9Xc967JwbwR9QiviaGPOlvdbotFoeHv+AX2af4Y0Y+KaU7zzdIA+SBa1BZkmUjF52Fi++UwUfefuf7gtff/XG89y4vp9vu9VTz8GvfPMnQBs/aAlfq7ZTwxTUNkNFRNClF5S3S1KFR9nGz5sF4i7vdpjPC3gmWToUVbD24Hf+jeiYSUXJnQOwsLMhI61sq5d/UI2VfHjnwvKVz7+u2zYVp3WaW3mv+f12zIv1rX+5E1ik9InTEkTfjt/S29ei37ApVzS7jx/m+Bx6/jjv7w7xAkhSgcpSYsyIacJ0l5rXJGXG/piotFQ38+ZFtXcmbn5PI7W5mQucM7v34ga3g6MXXUix+t81rUmo5Yf5+o9w4VDYh6kcDoyhgN5zEV+OzYJBytzg5nW0qrq0xYNyU6qVkfopH8BODWhXbZLjL41/wAJyVo++PMoL9avkGs+hBClgwRpUSaY5DKNqbmpWmHUJ7QSoLZzA/oJTdI0qeyKiYmGze+3JEWro83X2wz2f9OjNkHejtleY/OZW2w+cyvbfRndik0iIVnLjAwl7h+2X+Tk9Rh+2X2JlxtWYOf5O7Sv6cmbLSrr01zJ8FAQ/SAZa4usvdkzr9Udm5iCguHsbwB345Nxtsl+bnYhhHEpVdXdkyZNQqPRMGzYsJLOijAyGYdu5Vda8AaY2C0Yk4dF60rlbLNdPKSapz0VXW2wNEs/rpxdzr3KmwWUY9XgUINt607cpOO3O1h7IlK/bV/4Xb7eeJa78cnM3HyBw1eimbjmNL/suqRPk3Eo2guzdvPrbnXfvfhkLt5S92WcTyVt/vHWX20l8eFYcoADl+9S95MNjP/rZI75FkIYj1ITpPfv38/3339PrVq18k4snjhDWgcQ6GnP2E418k78UHWv9OFdLzf0NdhnZpr1T8PKzBQzUxOSMpRYX2/mb5BmaOsAVg0OpUttbyY9X4taPk4G+3/eGZ7v/I1ddYKkVDXAnroRo99+LfoBY1ae4OT1GJ75ehvtpm3nevQDg4lfomITuRb9gFuxSey+kL7MZ9oMa/MyPAAIIYxXqajujouLo2fPnvzwww98+umnJZ0dYYTc7a1YO6x5gY5pG+TJ6I41qOWTfRV2Zmnzjjev6sa2s7eo4eXAgGb+nLgeQ8SdeN5rU42mVcphYqJhWo86Bb6H7FyIiqdSOVvmZhPc/7f8mH6Vrq/Wn9VPdQoQnZDe5r317C1aBarDvCzMSs1zuRCCUlKSHjRoEB06dCAsLKyksyLKkLT5wxtUdMk7Meo0pACfdanJm839+bV/Q0xNNHz7ch1WDm5K86pu+irzjDIv21kQZ27GcPF2HPcSsi7pefhKtP73pQcNZzzLODPbvF2XuHI3AcCgqj7zhCz5EZOYwvRN54i4k1DgY4UQBWf0QXrx4sUcPHiQiRMn5it9UlISMTEx+p/Y2NhizqEoq5YNbEKzgHL612nLbFZwsWHks9UpZ5e/Fbm616/AsoFN9K+HP1NVH/Dzcu5mnEGpOL9GLjtm8LrZF5uJTkg2mN/8yt0HmQ/L0/+WHWPqhrO88Vv2i40IIYqWUQfpK1euMHToUBYsWICVVdaVlLIzceJEHB0d9T81auS/jVKIjOr6OtOvaSX9a8tHqCoOLp9epd42yJP5rzciyNuBxW88letx3225QM8f9xb4eteiswbgb/89T1SGJTxv3E9Ps2T/FVpN2cL5qDjik1J567cD2Y63Xn30BgCnI+XhV4jHwajbpA8cOEBUVBR169bVb9NqtWzbto0ZM2aQlJSEqalhiWTkyJEMHz5c//ratWsSqEWh2Vum/4k8ypAlc1MTFg5oRFRMkn4+8r+HNMs2bb/QSgR5O/DeH0cKfb3srDpyncTk9J7efx+9wchlx/BwsOLYNXWO8klrThFc3om1JyJZeyKSS3fi6d/Uv9jmRhdC5M6og3Tr1q05dsyw2q5v374EBgYyYsSILAEawNLSEkvL9GrImJiYLGmEyK96fs50r+9DxWyGZBVUk8rlst0+qFVlZm6+wK/9GlLdywFXW4ssM5rlpLyTNeamGi7dSaCcnQW345JzTHsrQykaYNmhawAGpeur9x6w8VSU/vXMzRfYdCqKtcOak5BsuBTo9nO3aBbgZrAtMUXLhpM3aeTvop8VTghReEYdpO3t7alZs6bBNltbW1xdXbNsF6I4aDQavngh9+U2H9X7barRL7QSrhnauL0c8xfgzE01rBgUyunIWJ7yd2Xl4WsMXXy40HnJrhr7dGQsqVodN+4brq3d66d9XJrUQf96/YlI3vhNnUPdwtSE/R+H6ZcgFUIUjlEHaSGeBBqNxiBAA3g45C9IxyVpcbKx4Cl/VwA61y5PcHlHdp6/TYCHPVfvPeDynXg2nooyGGtdUKOWH+f3bNqo+87dh4utJfcfpLDx1E399mStjog7CQTnY3hbQnIqc3deol1NTyq72RU6j0KURaUuSG/ZsqWksyBEsbMwM+GTzkHcjkvmm03nckyXcQKTNP5udvhnCnZPB7rT9btdOZ6nspstF27lvHhHdgEayHUq1F0XbvPr7ks0rORCilbhlUa+HLh8FytzU67ee0DDii4421owac1pft19mbk7w/nv42dyPJ8QT6JSF6SFeFL0alwRUOcU//PAVbaevaVvP/68azBjVh7ny3yOwa7j65zjPnd7S/56pymv//Ifux7OThbi48iRq/cfKf8T16hzo/9xQB3DXdHVhlcy9FSv4eXAikGhrDmuTpGasT394xXH2H7uNisHheJkI53WxJPLqIdgCSGgkb8rX74Yop/xDOCVRr6cmNCW1tU98n2e/g+Hk2XsqX3us/Zs+7AVNhZm9AtNH25W1y/noF7Nwz7HfbnZd+muweuTN2Ko+vGaLB3aElO0zN8TweU7Caw5HsnVewkyeYp4YklJWohSQptpgWpLs/xNiJLmw3bVaFHVjdq+ToxdeYImlV0xNzUhLfZn7MHeKcSbhCQtzrYW3I5LYtXh6yRr1TnLv3m5Nm/+doDLOQTOSuVss10j+/SNvMdWrzpyHe8MneYu3opj5LJjOFiZsfd/Ydku0SlEWSYlaSFKiSkvhmBqouHjDtULdbylmSnNq7rhYGXO1y/VzrLmdAWX9OUvXW0tmPxCLT5qH8iUF0M4Pr4tzQLK0espPwI9HVg44ClerOeT5RpPB7rTqFL206weupL3sLIhiw4ZTHf6w3Z1zvKYxFSuRasPBXFJqfy4/SLXox+QqtVx9mYsSjZt85vPRPHximP6VcDuP0jh6a+2MP4vdb1wrU5h6oaz7Dp/O898CVFSpCQtRCnRuLIrJye0LXAJOr8szUyZ1C2YO/HJ+Lkajgu3MDPht/6N9K/LO1nTN7SSvr05jZejVZZ1rdPcjEmv1v6sa01GLT+ebbqzN7MvcUfeT6KKuz2L90Xw6d+n+PTvU3SrW55lB68xqVswPR6uZHY7Lgl7KzP6zt0PgJ+LLQOa+7Pr/G0u3orn4q14Braswj/HbjB90zmmg8FQMiGMiQRpIUqR4grQaXpkWrIzN66Z1tK2tzLj9Wb+fL3hrH7bvL4N6PMwWKZpX9OTVxr64uVoRb95WecAX/Lf1SzbAGZsPsf0f89hkWEZ0WUH1QlZpm08R4+Gvvy2+xKjV54wWA/84u14omITuZSher7BZxsxNy38DHJCPC5S3S2EKBTXDB3QytlZcGxcWyqVs9WvgQ3Qspo7x8e3NTiunp8zGo2GpwM9eL9N1RzP/2mXmgxuVYWn/NXq8z0X77Iv/C47sqmedrIxZ862C4xeqVZlX8zQJq7TKbw8Zw+T1542OCZFm15FntberygK4/86wa+7L+V1+0I8FhKkhRCFYpahRJuQYU7wzBOx2Fma8VpjPwC61inPyxlK62+1qMzyTKuNgfoA8GJ9H95vWy3X4WNpUrQ6Pv/ndLb74pJScx0DDhAVm8jPO8JZcfgac3deYszKE5y7GcuGkzdzPe5R6HQKBy7fzTLdqhAZaZTselyUIVevXqVChQpcuXIFH5+sHV2EEIV3+Eo0QxcfYtSz1WkT5Amoa1mPWHqUF+v56LflJTlVx3+X7/LXkRvYWJjSp0lFKrjYAOoc4b1/3oeJRkOqruBfV/7lbA1K1l6OVlmmOH31KV/m74nI9vj17zbHw8GqyKc4/W3PZUavOE7TKuWY/3qjvA8QRq844o0EaSGE0YtPSiUuKZVGn28C1LHed+NzXkwkN70b+/HL7ssG29ztLQ0WGsmorq8TByOimf1qPbQ6heWHrvHlC7VwsjFn3q5LrDpynbDqHvRpUhFby/x382nz9VbO3owDpONaWVEc8UY6jgkhjJ6tpRm2lmZs/7AVyVodLjYWtPtmm0GP8Yx6NKjA4v3ZT2XqamfJZ11rcvN+Irsu3OG/y/dyDNAAByOiAViw9zLbz6nt4b/tuUx9P2fG/3USgEMR0dyOS2Jsp6B835MG6bgm8iZBWghRaqRVgQP81r8R8/dc5teHpWI3e0v97GXD21SlQUWXbNfktrEwpWcjtY38wsKD+b52dEKK/vc1xyOZmqEXO8D6Ezfxd7Nj9Irj+LnaMKCZP36uNlmW80yT0/LkiqLw5boz+LvZ8UI2Y9HFk0U6jgkhSqWqHvYMbR2gf92ianowdLax4Pl6PjjbZG1HTps5DSjQqlvHrqXPZZ7dimLJWh2jV6hjvy/fSeDjFcfp9dM+Lt9Jbw9PTtUxctkx1hy7gSZDlNbpFC7cikNRFPaF3+W7LRd4/48jLMmhNkA8OSRICyFKLRdbCzoEe9EpxJvqXg767eYPe57/2q8RzQLKEVbdXb8vMUNP9Cru6UHa0syE5lXdcizh5iXzHORpMk6RuvLwNRbti+DtBQcNKru/2XSO1l9tZe7OSwZV7x8uPcr16AeFy5AoEyRICyFKLY1Gw8yedfn25TpUKmeTZX+wjyO/9W/Ej70b6LdVzhCYK7ulT3ryXpuq/NqvIYsHPFWkeRy44CBjVh6n63c7DcZ4Z1xmNG050gmrT5KqM5yxrf0321l7PDLbqU9F2Sdt0kKIMqFVNXc+bFeNIG/HbPevHBTK/kt36VTLW7+tuqcD3eqUx9HGnP5N/QF11bHlA5uwaF9EjrOfFURCslbfbn7oYSc0IMfOanGJhuOm7z9I4a35B3C2Mee7nvV4yt+FH7ZfpJqnA8mpOhbti6CKux3vtala7DPSicdPgrQQokzQaDQMbFklx/0hFZwIqeBksM3ERMPUl2pnSVvH15mYxNQ8g/TvbzyFr6sNjSf+W+D8ZjeEzNxUw934lGxSw72EFIb9foip3Wtnmbjl39NReDla0TfDcqPZURSFE9djqOJuZ7D0qTBeUt0thBDZaFLZVR/Uu9fPvpe1q50FHvZWtKnhQQUXayY/H4ztIyynmaJV+Hrj2Rz3JyRribib/RKh1+6lt10firjHu78fzvIgsOzgNTp+u4O35x8odB7F4yUlaSGEyIa5qQl/vtWYq/ceUMHZGitzU45du29QZe1kY4GJiYY5r9XXb+tYy5tnpm7leqZZzYqCqYnGoC07Izsr9etcURS6frcLUHu5j+lUQ5/ml4dzkm8+cytf14u8n8jUDWfo3aRijs0IonhJkBZCiByYm5pQ6eGKWhM61wTU4VLDfj8MGC4yksbW0owfezfg2enbC3StBhWd2X8p9zW3oxNSclzi80GKlpmbzxuM305bgzuNpVnOladX7yVgbW6Kq52lftuIpUfZevYWKw9f58yn7fNzG6KISZAWQogCMDHRMP3lOrmmqeHtQPjEZzl27T57L97luy3nGdDcny/WnjFIl3E60iVvNuZufDJanULDh9OfFkT4rXjWZ1oQ5E5cenX3zM3nDR4CbsYk8tOOcBytzVEUhSnrz1LZzZZN77XUpzkYoaZPymGNcIBNp27y7+koXmpQAWcbC2ZvvUD/ppXwL8AYdJEzCdJCCFEMNBoNtXycqOXjxIDm/py6EZMlSNcs78i/p6P06dNKsU2rlMt2Sc7cZA7QoAbZNcduMHbViSy9yRtl8yBw4VY8ZyJjGbnsKF6O1sRm6mmuKAqKoj6oANyOS6L/L+qa4Mev3SdVp3ZM23w6il0jWxco/yJ70nFMCCEeg3IZqpHTjGwfSIC7HcOfMVxXe8YrdfjqxRD967EZ2pUzm/FKzqV6nQJvLziY69zkmX2y+iQHI6L5+9gNg+2pWh0dv91B1+92onu4GtnKw9f1+49eu8+J6+pMbJnb4yf8dZKnp2whOqFwi6I8yaQkLYQQj4FLNu3Xld3s2DC8RZbtTg+nNQ0q78Cl2wm0q+lJ0yrlmLPtIn8cSB8WZmlmgpmJYVlr5aBQFNSS7ccrsm+/zk1OJfh9l+7qg/DGUzdpE+TJzZj0YJy5P9vVewn4ONsQcSeBn3eGA7D7wh3aB3sVOE9PMilJCyHEY2BqYjjf6JiONfTVxjkJ9HSgXU11Te4AD3ueq+1tsN/XxYYGFZ2xf7hE5oxX6hBSwYnaFZyo6+uc67mHhQWwbGAT6vg65Sv/vX/ep//9jd8OsOrIdeKSUnNM3+LLLXz+zymaf7lZvy3jvOkAm09H0WTiJvaF381XHp5EEqSFEOIx8Xa0AmD1O03p1zT3iUey07RKOX58rT69G/sR4uPIjFfq4mpnyf6Pwzj7aXs6ZphNLeO85GM61qCqR/rrNjU8eOfpAOr6OvNm88pZrtOksmuWbSlaw6LykEWHOHIlGgAfZ+ss6bU6hTnbLhpsW7A3gq1n04d/9Z23n+v3E3njt/+yHB9+O57F+yJI1ebcae1JINXdQgjxmKx9tzmR9xOp6mFfqOM1Gg1hNTwIq+FhsD272cMszEyY9lJtbsYk0je0Io38XegwfQcAb7WsrC/Ztw3y4KsXQ7h8N4HpD+cQbxvkib+bLaduxJKi1XH06v0s5wf01d+hlcvx+395r9i1L/wu+8L38feQplyPTq8qz7gMaIpWx6jlx/SzvSVrdfR6yo+78cmciYylcWVX7sYn42JrYbCSWFklQVoIIR4TBytzHKyyLp9ZXLrUKa//3cPBSv+7VYY5vjUaDc/X82H3hTv6IO3paEXvJhUBWPLfFT7882iu12lWtRy7L97JcTa0zNIeFjJKSE5l9dEbfLziOMkZhnyNWXmC3x8u2XniegydQrz568h1RrQL5O2WWWsByhqp7hZCiCeAi016xzV7q6zlM3eH9N7nGQN6oGfepX47SzPeb1vNYFs1D3uGP1OVIa0D6JKpLT07m0/f4sM/jxoE6DQnrsfoS+1/HVF7lE9ee1qfNkWr488DV3Nc1lNRlFJbbS5BWgghngAmJhpmv1qPz7sGU8El67KeGQOzs016ab9iOdssaQHeb5M+bMzO0gy3TEPMvJ2sGNI6gOHPVM2ysEl2Pv/nVJ5pMtv5sCf6on0RvP/HEbp+tzNLGkVReHH2blpP3UpSqjbLfmNn1EF64sSJNGjQAHt7e9zd3enSpQtnzpzJ+0AhhBBZtKvpySuNfLPdZ2dpxssNK9Cxlhe+GYJ4dtXzzQLK4Z4hqNtZmeHnahj4Bz8dkOs50lRwscbeyoxrOZSCc9N33n62nb3FTzvUIV43Y7KOB78dl8x/l+9x+U4CF6LiiUtKJaUUlaqNuk1669atDBo0iAYNGpCamsr//vc/2rRpw8mTJ7G1zf7pTgghROFM7FYr1/1jO9XA08GK0IByBguN2FqY4e1kzXc963IrNol6fs7ULJ++IIeDdXqQ7lanPMsOXdO/blPDk3sJySw7mL6tIF7LMDQMYMn+K5iYaNhx7hb9m/oTm5jeKW33xTv0mLObSm52rBwUWqjrPW5GHaTXrl1r8HrevHm4u7tz4MABmjdvXkK5EkKIJ8vG4c3ZdeEOrzT0xcxUrYDNuLhIWhv3szlMVBLoaY+ZiYZK5WwZ+Wx1fZC2szRjWFgAv++/wjIKF6Qz+3Bpeie3ewkptKrmpn/9yeqTABy5Ek18Uiq2lkYdAgEjD9KZ3b+vDgNwcXEp4ZwIIcSTo4q7PVXcDTuQeTqmV3fnFewquNiwb1QYDlZmmJma0De0ItvO3mLZ26HYW5nTpHK5fOele30fLMxMqObpwOg8ZlS7FZvEoYdjuTP79t/zKCi80tCXhGQtgZ72RjmkS6MoOSxOamR0Oh3PPfcc0dHR7NiRtft+mqSkJJKS0tslrl27Ro0aNbhy5Qo+Ptkv3C6EEKLg1h6PxMJMw9OBHnknzsPfR28wd2c4df2cs0yC0qGWF38fVecSvzSpg3578Lh1BouAvNbYj193X9a/dnrYAS7jOOycvNLIl8+7Bj/SPVy9epUKFSoUabwpNSXpQYMGcfz48VwDNKidzcaPH/+YciWEEE+utClLi0KHWl50qKVWlw9sWZnaEzbo9416tjpV3e15OtDd4JiMAfqj9oHU83M2CNL5Cc5pGlTMfRrVkmLUvbvTDB48mNWrV7N58+Y8n05GjhzJ/fv39T8nT558TLkUQghRFJxsLPiuZ139a3d7S4aGBRDs45jjMW+1qEygpz2O1ub6EnSa5lXdcjgqXatq7nmmKQlGHaQVRWHw4MEsX76cf//9l0qV8p7r1tLSEgcHB/2PvX3hpt8TQghRcjIG2rTOapmNbB+IiQZ+698QAHsrc9YNa87GDCuLBZd3ZPLz6dXYg1tVoZyd4YpkbvaWONlkXaXMGBh1dfegQYNYuHAhK1euxN7ensjISAAcHR2xts46obsQQoiyobG/K0OerkI1T4cc07zR3J9Xn/Iz6LiW1qGtYy0v1p+8yZQXQ/BytKZFVTf2ht/h+Xo+LD2YvtynhZkJs1+tm+XcxsKoO47l1NNu7ty59OnTJ1/nKI6GfCGEEMYtOVVHXFKqfh3vB8la4pNTKWdnyU87wvlk9UlaB7ozs2fdbBcoKYwnruOYET8/CCGEMGIWZia4mKVXYVtbmGJtoQbjPk0qEuhpT0gFpyIL0MXFqIO0EEIIUdRMTTSEVsn/2OySZNQdx4QQQognmQRpIYQQwkhJkBZCCCGMlARpIYQQwkhJkBZCCCGMVJnv3a3TqYt737hxo4RzIoQQoixLizNpcacolPkgffPmTQAaNmxYwjkRQgjxJLh58ya+vr5Fci6jnnGsKKSmpnLo0CE8PDwwMXm02v3Y2Fhq1KjByZMnZU7wPMh7lX/yXuWfvFf5I+9T/hXle6XT6bh58yZ16tTBzKxoysBlPkgXpZiYGBwdHbl//z4ODjnPJyvkvSoIea/yT96r/JH3Kf+M/b2SjmNCCCGEkZIgLYQQQhgpCdIFYGlpydixY7G0tCzprBg9ea/yT96r/JP3Kn/kfco/Y3+vpE1aCCGEMFJSkhZCCCGMlARpIYQQwkhJkBZCCCGMlATpfJo5cyYVK1bEysqKRo0asW/fvpLOktGZOHEiDRo0wN7eHnd3d7p06cKZM2dKOlulwqRJk9BoNAwbNqyks2KUrl27xquvvoqrqyvW1tYEBwfz33//lXS2jI5Wq2X06NFUqlQJa2trKleuzCeffIJ0PYJt27bRqVMnvL290Wg0rFixwmC/oiiMGTMGLy8vrK2tCQsL49y5cyWT2QwkSOfD77//zvDhwxk7diwHDx4kJCSEtm3bEhUVVdJZMypbt25l0KBB7Nmzhw0bNpCSkkKbNm2Ij48v6awZtf379/P9999Tq1atks6KUbp37x6hoaGYm5uzZs0aTp48yVdffYWzs3NJZ83oTJ48mVmzZjFjxgxOnTrF5MmT+eKLL/j2229LOmslLj4+npCQEGbOnJnt/i+++ILp06cze/Zs9u7di62tLW3btiUxMfEx5zQTReSpYcOGyqBBg/SvtVqt4u3trUycOLEEc2X8oqKiFEDZunVrSWfFaMXGxioBAQHKhg0blBYtWihDhw4t6SwZnREjRihNmzYt6WyUCh06dFD69etnsK1bt25Kz549SyhHxglQli9frn+t0+kUT09P5csvv9Rvi46OViwtLZVFixaVQA7TSUk6D8nJyRw4cICwsDD9NhMTE8LCwti9e3cJ5sz43b9/HwAXF5cSzonxGjRoEB06dDD4/yUMrVq1ivr16/Piiy/i7u5OnTp1+OGHH0o6W0apSZMmbNq0ibNnzwJw5MgRduzYQfv27Us4Z8YtPDycyMhIg79DR0dHGjVqVOLf82V+FaxHdfv2bbRaLR4eHgbbPTw8OH36dAnlyvjpdDqGDRtGaGgoNWvWLOnsGKXFixdz8OBB9u/fX9JZMWoXL15k1qxZDB8+nP/973/s37+fIUOGYGFhQe/evUs6e0blo48+IiYmhsDAQExNTdFqtXz22Wf07NmzpLNm1CIjIwGy/Z5P21dSJEiLYjFo0CCOHz/Ojh07SjorRunKlSsMHTqUDRs2YGVlVdLZMWo6nY769evz+eefA1CnTh2OHz/O7NmzJUhnsmTJEhYsWMDChQsJCgri8OHDDBs2DG9vb3mvSimp7s5DuXLlMDU11a9LnebmzZt4enqWUK6M2+DBg1m9ejWbN2/Gx8enpLNjlA4cOEBUVBR169bFzMwMMzMztm7dyvTp0zEzM0Or1ZZ0Fo2Gl5cXNWrUMNhWvXp1IiIiSihHxuuDDz7go48+okePHgQHB9OrVy/effddJk6cWNJZM2pp3+XG+D0vQToPFhYW1KtXj02bNum36XQ6Nm3aROPGjUswZ8ZHURQGDx7M8uXL+ffff6lUqVJJZ8lotW7dmmPHjnH48GH9T/369enZsyeHDx/G1NS0pLNoNEJDQ7MM5Tt79ix+fn4llCPjlZCQgImJ4de6qakpOp2uhHJUOlSqVAlPT0+D7/mYmBj27t1b4t/zUt2dD8OHD6d3797Ur1+fhg0bMm3aNOLj4+nbt29JZ82oDBo0iIULF7Jy5Urs7e31bTmOjo5YW1uXcO6Mi729fZa2eltbW1xdXaUNP5N3332XJk2a8Pnnn9O9e3f27dvHnDlzmDNnTklnzeh06tSJzz77DF9fX4KCgjh06BBTp06lX79+JZ21EhcXF8f58+f1r8PDwzl8+DAuLi74+voybNgwPv30UwICAqhUqRKjR4/G29ubLl26lFymQYZg5de3336r+Pr6KhYWFkrDhg2VPXv2lHSWjA6Q7c/cuXNLOmulggzBytlff/2l1KxZU7G0tFQCAwOVOXPmlHSWjFJMTIwydOhQxdfXV7GyslL8/f2VUaNGKUlJSSWdtRK3efPmbL+fevfurSiKOgxr9OjRioeHh2Jpaam0bt1aOXPmTMlmWlEUWQVLCCGEMFLSJi2EEEIYKQnSQgghhJGSIC2EEEIYKQnSQgghhJGSIC2EEEIYKQnSQgghhJGSIC2EEEIYKQnSQgghhJGSIC2EeCQajYYVK1aUdDaEKJMkSAtRivXp0weNRpPlp127diWdNSFEEZAFNoQo5dq1a8fcuXMNtllaWpZQboQQRUlK0kKUcpaWlnh6ehr8ODs7A2pV9KxZs2jfvj3W1tb4+/vz559/Ghx/7Ngxnn76aaytrXF1deWNN94gLi7OIM3PP/9MUFAQlpaWeHl5MXjwYIP9t2/fpmvXrtjY2BAQEMCqVav0++7du0fPnj1xc3PD2tqagICALA8VQojsSZAWoowbPXo0zz//PEeOHKFnz5706NGDU6dOARAfH0/btm1xdnZm//79/PHHH2zcuNEgCM+aNYtBgwbxxhtvcOzYMVatWkWVKlUMrjF+/Hi6d+/O0aNHefbZZ+nZsyd3797VX//kyZOsWbOGU6dOMWvWLMqVK/f43gAhSrOSXoZLCFF4vXv3VkxNTRVbW1uDn88++0xRFHX50LfeesvgmEaNGilvv/22oiiKMmfOHMXZ2VmJi4vT7//7778VExMTJTIyUlEURfH29lZGjRqVYx4A5eOPP9a/jouLUwBlzZo1iqIoSqdOnZS+ffsWzQ0L8YSRNmkhSrlWrVoxa9Ysg20uLi763xs3bmywr3Hjxhw+fBiAU6dOERISgq2trX5/aGgoOp2OM2fOoNFouH79Oq1bt841D7Vq1dL/bmtri4ODA1FRUQC8/fbbPP/88xw8eJA2bdrQpUsXmjRpUqh7FeJJI0FaiFLO1tY2S/VzUbG2ts5XOnNzc4PXGo0GnU4HQPv27bl8+TL//PMPGzZsoHXr1gwaNIgpU6YUeX6FKGukTVqIMm7Pnj1ZXlevXh2A6tWrc+TIEeLj4/X7d+7ciYmJCdWqVcPe3p6KFSuyadOmR8qDm5sbvXv3Zv78+UybNo05c+Y80vmEeFJISVqIUi4pKYnIyEiDbWZmZvrOWX/88Qf169enadOmLFiwgH379vHTTz8B0LNnT8aOHUvv3r0ZN24ct27d4p133qFXr154eHgAMG7cON566y3c3d1p3749sbGx7Ny5k3feeSdf+RszZgz16tUjKCiIpKQkVq9erX9IEELkToK0EKXc2rVr8fLyMthWrVo1Tp8+Dag9rxcvXszAgQPx8vJi0aJF1KhRAwAbGxvWrVvH0KFDadCgATY2Njz//PNMnTpVf67evXuTmJjI119/zfvvv0+5cuV44YUX8p0/CwsLRo4cyaVLl7C2tqZZs2YsXry4CO5ciLJPoyiKUtKZEEIUD41Gw/Lly+nSpUtJZ0UIUQjSJi2EEEIYKQnSQgghhJGSNmkhyjBpzRKidJOStBBCCGGkJEgLIYQQRkqCtBBCCGGkJEgLIYQQRkqCtBBCCGGkJEgLIYQQRkqCtBBCCGGkJEgLIYQQRkqCtBBCCGGk/g/5Ff3USiuBlgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "- Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences\n",
    "- However, based on the training and validation set losses, we can see that the model starts overfitting\n",
    "- If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -- it simply memorizes the training data\n",
    "- Later, we will cover decoding strategies that can mitigate this memorization by a certain degree\n",
    "- Note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times\n",
    "  - The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text\n",
    "  - Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380c42-b31c-4ee1-b8b9-244094537272",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713235-1561-467f-bf63-bf11ade383f0",
   "metadata": {},
   "source": [
    "**If you are interested in augmenting this training function with more advanced techniques, such as learning rate warmup, cosine annealing, and gradient clipping, please refer to [Appendix D](../../appendix-D/01_main-chapter-code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cdf2f-09a5-4eb0-a20a-d7aac5c14c2c",
   "metadata": {},
   "source": [
    "**If you are interested in a larger training dataset and longer training run, see [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f45fc-bf78-42f2-bd24-2355db41b28f",
   "metadata": {
    "id": "699f45fc-bf78-42f2-bd24-2355db41b28f"
   },
   "source": [
    "## 5.3 Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9086e-2c27-41da-97d0-49137d0ba3c7",
   "metadata": {},
   "source": [
    "- Inference is relatively cheap with a relatively small LLM as the GPT model we trained above, so there's no need to use a GPU for it in case you used a GPU for training it above\n",
    "- Using the `generate_text_simple` function that we used earlier inside the simple training function, we can generate new text one word (or token) at a time\n",
    "- As explained in section 5.1.2, the next generated token is the token corresponding to the largest probability score among all tokens in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2734cee0-f6f9-42d5-b71c-fa7e0ef28b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Maintenance costs are a major components (i.e., Fort at various machines, and other insol%) of the rotor to the rotor, the rotor speed\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Maintenance costs are a major\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25dbe31-bb7c-4893-b25b-47d0492d4aa4",
   "metadata": {},
   "source": [
    "- Even if we execute the `generate_text_simple` function above multiple times, the LLM will always generate the same outputs\n",
    "- We now introduce two concepts, so-called decoding strategies, to modify the `generate_text_simple`: *temperature scaling* and *top-k* sampling\n",
    "- These will allow the model to control the randomness and diversity of the generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6f380-a798-4fd9-825c-17b7cd29a994",
   "metadata": {},
   "source": [
    "### 5.3.1 Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f53c-0612-43d3-aa82-52447eac50fa",
   "metadata": {},
   "source": [
    "- Previously, we always sampled the token with the highest probability as the next token using `torch.argmax`\n",
    "- To add variety, we can sample the next token using The `torch.multinomial(probs, num_samples=1)`, sampling from a probability distribution\n",
    "- Here, each index's chance of being picked corresponds to its probability in the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7531bae-d5de-44c0-bc78-78fed077e22a",
   "metadata": {},
   "source": [
    "- Here's a little recap of generating the next token, assuming a very small vocabulary for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01a5ce39-3dc8-4c35-96bc-6410a1e42412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"major\": 0,\n",
    "    \"cost\": 1, \n",
    "    \"maintenance\": 2, \n",
    "    \"machine\": 3,\n",
    "    \"component\": 4,\n",
    "    \"rotor\": 5, \n",
    "    \"speed\": 6,\n",
    "    \"various\": 7,\n",
    "    \"predictive\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"Maintenance costs are a major\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6400572f-b3c8-49e2-95bc-433e55c5b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b23b863e-252a-403c-b5b1-62bc0a42319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x major\n",
      "0 x cost\n",
      "0 x maintenance\n",
      "582 x machine\n",
      "2 x component\n",
      "0 x rotor\n",
      "0 x speed\n",
      "343 x various\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0a27-830b-42b5-9986-6d1a7de04dd9",
   "metadata": {},
   "source": [
    "- Instead of determining the most likely token via `torch.argmax`, we use `torch.multinomial(probas, num_samples=1)` to determine the most likely token by sampling from the softmax distribution\n",
    "- For illustration purposes, let's see what happens when we sample the next token 1,000 times using the original softmax probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7d9cf-a26d-4d9a-8664-4af1efa73832",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via a concept called temperature scaling\n",
    "- \"Temperature scaling\" is just a fancy word for dividing the logits by a number greater than 0\n",
    "- Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax\n",
    "- Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0759e4c8-5362-467c-bec6-b0a19d1ba43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e66e613-4aca-4296-a984-ddd0d80c6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750e989-842a-4cfa-a44b-cf44d6e49163",
   "metadata": {},
   "source": [
    "- We can see that the rescaling via temperature 0.1 results in a sharper distribution, approaching `torch.argmax`, such that the most likely word is almost always selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4600713-c51e-4f53-bf58-040a6eb362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x major\n",
      "0 x cost\n",
      "0 x maintenance\n",
      "985 x machine\n",
      "0 x component\n",
      "0 x rotor\n",
      "0 x speed\n",
      "15 x various\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e93cb-8e2a-42a1-b1ba-4fd5fe64c26b",
   "metadata": {},
   "source": [
    "- The rescaled probabilities via temperature 5 are more uniformly distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dfb48f0-bc3f-46a5-9844-33b6c9b0f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 x major\n",
      "75 x cost\n",
      "42 x maintenance\n",
      "239 x machine\n",
      "71 x component\n",
      "46 x rotor\n",
      "32 x speed\n",
      "227 x various\n",
      "103 x predictive\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83f0c4-3774-4375-ad7f-96440ba5fef7",
   "metadata": {},
   "source": [
    "- Assuming an LLM input \"Maintenance costs are a major\", using the approach above can sometimes result in nonsensical texts, such as \"Maintenance costs are a major speed\", 3.2% of the time (32 out of 1000 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4873e-07e4-4abb-85df-bdaedcc1a6f7",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4da95a-8bb2-4f69-a9b0-a643531db5df",
   "metadata": {},
   "source": [
    "- To be able to use higher temperatures to increase output diversity and to reduce the probability of nonsensical sentences, we can restrict the sampled tokens to the top-k most likely tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6fffd-2730-4abe-a2d3-781fc4836f17",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/topk.webp\" width=500px>\n",
    "\n",
    "- (Please note that the numbers in this figure are truncated to two\n",
    "digits after the decimal point to reduce visual clutter. The values in the Softmax row should add up to 1.0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba12da5-6ff1-4008-91b8-d2d537cbc14c",
   "metadata": {},
   "source": [
    "- In code, we can implement this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a7f908a-e9ec-446a-b407-fb6dbf05c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "753865ed-79c5-48b1-b9f2-ccb132ff1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4844f000-c329-4e7e-aa89-16a2c4ebee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56056503-a15d-4315-a3ff-46647a4c7c45",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34770423-473d-46f6-a5fa-6b2979564d26",
   "metadata": {},
   "source": [
    "- The previous two subsections introduced temperature sampling and top-k sampling\n",
    "- Let's use these two concepts to modify the `generate_simple` function we used to generate text via the LLM earlier, creating a new `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e318891-bcc0-4d71-b147-33ce55febfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa2a0d7d-0457-42d1-ab9d-bd67683e7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Maintenance costs are a major expenses to a clear. This capability effect of imbalance between the equipment and defect\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Maintenance costs are a major\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2002ca-f4c1-48af-9e0a-88bfc163ba0b",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52676-f026-4566-a226-2a90269f9d53",
   "metadata": {},
   "source": [
    "- Training LLMs is computationally expensive, so it's crucial to be able to save and load LLM weights\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model-3.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4c7f9-592f-43d6-a00e-598fa01dfb82",
   "metadata": {},
   "source": [
    "- The recommended way in PyTorch is to save the model weights, the so-called `state_dict` via by applying the `torch.save` function to the `.state_dict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d67d869-ac04-4382-bcfb-c96d1ca80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e889e0-07bf-43e5-8f92-5c5c7aeaad9e",
   "metadata": {},
   "source": [
    "- Then we can load the model weights into a new `GPTModel` model instance as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d57d914-60a3-47f1-b499-5352f4c457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa81aec-9c72-4f46-8ae2-4a4fde3edbc1",
   "metadata": {},
   "source": [
    "- It's common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD\n",
    "- These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bbd175bb-edf4-450e-a6de-d3e8913c6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a0c7295-c822-43bf-9286-c45abc542868",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194350e-0409-4a63-8ffd-d3a896509032",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6c38-7278-40e0-bd9f-8a2b1feac3ec",
   "metadata": {},
   "source": [
    "- Previously, we only trained a small GPT-2 model using a very small short-story book for educational purposes\n",
    "- Interested readers can also find a longer pretraining run on the complete Project Gutenberg book corpus in [../03_bonus_pretraining_on_gutenberg](../03_bonus_pretraining_on_gutenberg)\n",
    "- Fortunately, we don't have to spend tens to hundreds of thousands of dollars to pretrain the model on a large pretraining corpus but can load the pretrained weights provided by OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddbdb-3878-4669-9a39-d231fbdfb834",
   "metadata": {},
   "source": [
    "- For an alternative way to load the weights from the Hugging Face Hub, see [../02_alternative_weight_loading](../02_alternative_weight_loading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cab892-a165-4f43-9601-f517bc212ab6",
   "metadata": {},
   "source": [
    "- First, some boilerplate code to download the files from OpenAI and load the weights into Python\n",
    "- Since OpenAI used [TensorFlow](https://www.tensorflow.org/), we will have to install and use TensorFlow for loading the weights; [tqdm](https://github.com/tqdm/tqdm) is a progress bar library\n",
    "- Uncomment and run the next cell to install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb9fdf02-972a-444e-bf65-8ffcaaf30ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0747edc-559c-44ef-a93f-079d60227e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "tqdm version: 4.65.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5bc89eb-4d39-4287-9b0c-e459ebe7f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76a736-6f9f-4328-872e-f89a7b70a2cc",
   "metadata": {},
   "source": [
    "- We can then download the model weights for the 124 million parameter model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76271dd7-108d-4f5b-9c01-6ae0aac4b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 26.4kiB/s]\n",
      "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:02<00:00, 488kiB/s]\n",
      "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 16.6kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [08:38<00:00, 961kiB/s]\n",
      "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 1.03MiB/s]\n",
      "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:01<00:00, 349kiB/s]\n",
      "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:01<00:00, 309kiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1a31951-d971-4a6e-9c43-11ee1168ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "857c8331-130e-46ba-921d-fa35d7a73cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c48dac94-8562-4a66-84ef-46c613cdc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e100c-294e-4afc-a70a-2f398ac4c104",
   "metadata": {},
   "source": [
    "- Alternatively, \"355M\", \"774M\", and \"1558M\" are also supported `model_size` arguments\n",
    "- The difference between these differently sized models is summarized in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f19d32-5aae-4176-9f86-f391672c8f0d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/gpt-sizes.webp?timestamp=123\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e5076-f08d-41fc-bd8b-1cfe53538f41",
   "metadata": {},
   "source": [
    "- Above, we loaded the 124M GPT-2 model weights into Python, however we still need to transfer them into our `GPTModel` instance\n",
    "- First, we initialize a new GPTModel instance\n",
    "- Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting `qkv_bias` to `True` in our implementation, too\n",
    "- We are also using the `1024` token context length that was used by the original GPT-2 model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9fef90dd-0654-4667-844f-08e28339ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f29ac-8342-4b3d-a57d-9b0166ced314",
   "metadata": {},
   "source": [
    "- The next task is to assign the OpenAI weights to the corresponding weight tensors in our `GPTModel` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9a92229-c002-49a6-8cfb-248297ad8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f22d5d95-ca5a-425c-a9ec-fc432a12d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7472cb-54dc-4311-96d8-b2694f885cee",
   "metadata": {},
   "source": [
    "- If the model is loaded correctly, we can use it to generate new text using our previous `generate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f690253-f845-4347-b7b6-43fabbd2affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Maintenance costs are a major sticking point. But it seems worth pointing out that this is a business by itself; what gets lost on that side is that\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Maintenance costs are a major\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d079f98-a7c4-462e-8416-5a64f670861c",
   "metadata": {},
   "source": [
    "- We know that we loaded the model weights correctly because the model can generate coherent text; if we made even a small mistake, the mode would not be able to do that"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
